{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eb0c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library imports\n",
    "\n",
    "#General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math, time, random, datetime\n",
    "import time\n",
    "\n",
    "#Data Exploration\n",
    "from ydata_profiling import ProfileReport\n",
    "import uszipcode\n",
    "from uszipcode import SearchEngine\n",
    "import sweetviz as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db181355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import plotly.express as px \n",
    "\n",
    "#import for interactive plotting\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d81af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning\n",
    "from sklearn import model_selection, tree, preprocessing, metrics, linear_model\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron,SGDClassifier,LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold, GridSearchCV, learning_curve, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693094bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the data\n",
    "df_grade = pd.read_csv(\"SOECS_clean_grade_data_2.csv\")\n",
    "df_address = pd.read_csv(\"SOECS_clean_pr_address_2.csv\")\n",
    "df_retention = pd.read_csv(\"SOECS_clean_retention_data_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf5ba24",
   "metadata": {},
   "source": [
    "##### Grade Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762a763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intial view of the first three rows of the data\n",
    "\n",
    "# Removes the limit for the number of displayed columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Sets the limit for the number of displayed rows\n",
    "pd.set_option(\"display.max_rows\",None)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_grade.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b99b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see unique quantities of each feature\n",
    "df_grade.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9119262",
   "metadata": {},
   "source": [
    "Will remove Camp_Desc in the data cleaning section as it only has 1 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e9d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the various different majors to compare to degrees\n",
    "value_counts = df_grade['Curr_1_1_Majr_Desc'].value_counts()\n",
    "print(value_counts.head(10))\n",
    "print(value_counts.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the different values for year term \n",
    "x = df_grade.Year_Term.unique()\n",
    "x=np.sort(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc134fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Comparing to degrees\n",
    "value_counts = df_grade['Curr_1_Degc_Desc'].value_counts()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41728fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the type of data and for null values\n",
    "df_grade.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d0649",
   "metadata": {},
   "source": [
    "There are null values in the last few columns that need to be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887bc39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the percentage of NaN rows in data\n",
    "num_rows_with_nan = df_grade.isnull().any(axis=1).sum()\n",
    "print(\"percent of rows with Nan:\",round(num_rows_with_nan/df_grade.shape[0]*100,3),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae31924",
   "metadata": {},
   "source": [
    "About 1% of the rows have Nan values, so they will all be removed in the data cleaning section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding an example where theres more than 1 Crse_Name for corresponding Crse_Title.\n",
    "\n",
    "# Count the number of unique values of crse_name for each crse_title\n",
    "crse_counts = df_grade.groupby('Crse_Title')['Crse_Name'].nunique()\n",
    "\n",
    "# Find the first crse_title with more than one or zero unique crse_name\n",
    "first_mismatch = crse_counts[(crse_counts != 1)].index[0]\n",
    "\n",
    "print(f\"The first crse_title with either more than 1 or 0 unique crse_name is {first_mismatch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1322e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking to see\n",
    "filtered_column = df_grade[df_grade['Crse_Title'] == 'Academic Skills for Success']['Crse_Name']\n",
    "\n",
    "# get the unique values in the filtered column\n",
    "unique_values = filtered_column.unique()\n",
    "\n",
    "# print the unique values\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c31bd7",
   "metadata": {},
   "source": [
    "There can be more than 1 Subj_Code per Crse_Title, so drop Crse_Title durring cleaning for being redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d966cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#There was more Grde_Code than the standard grades, so regular expression was used to find all the unexpected Grde_Codes\n",
    "\n",
    "# Get all unique values in the Grde_Code column\n",
    "all_grades = df_grade['Grde_Code'].unique()\n",
    "\n",
    "# Define a regular expression pattern to match non-letter grades\n",
    "pattern = r'[^A-Fa-f+-]'\n",
    "\n",
    "# Filter the list of grades based on the pattern\n",
    "non_letter_grades = [grade for grade in all_grades if re.search(pattern, grade)]\n",
    "\n",
    "print(f\"All non-letter grades in the Grde_Code column are: {non_letter_grades}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Num of Students durring each semester\n",
    "\n",
    "#removing the unwanted Year_Term values\n",
    "x2=x[6:24]\n",
    "df_grade = df_grade[df_grade['Year_Term'].isin(x2)]\n",
    "\n",
    "\n",
    "# Grouping by Year_Term and counting unique PIDM\n",
    "df_count = df_grade.groupby('Year_Term')['PIDM'].nunique().reset_index()\n",
    "\n",
    "# Creating the count plot\n",
    "plt.figure(figsize=(12, 6)) # Adjust the figure size if needed\n",
    "ax = sns.barplot(x='Year_Term', y='PIDM', data=df_count)\n",
    "plt.title('Total Unique PIDM count per Year-Term')\n",
    "plt.xlabel('Year-Term')\n",
    "plt.ylabel('Unique PIDM Count')\n",
    "\n",
    "# Adding count values at the top of each bar\n",
    "for i, row in df_count.iterrows():\n",
    "    ax.text(row.name, row.PIDM + 0.5, row.PIDM, ha='center', color='black')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d856a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to store the percentage reductions\n",
    "reductions = {}\n",
    "\n",
    "# Loop through the Year-Term values and calculate the percentage reduction\n",
    "for i in range(len(x2)):\n",
    "    if x2[i].endswith('S'):\n",
    "        year = int(x2[i][:-1])\n",
    "        count_s = df_count.loc[df_count['Year_Term'] == x2[i], 'PIDM'].values[0]\n",
    "        if f'{year}F' in x2:\n",
    "            count_f = df_count.loc[df_count['Year_Term'] == f'{year}F', 'PIDM'].values[0]\n",
    "            percentage_reduction = ((count_s - count_f) / count_s) * 100\n",
    "            reductions[f'{year}F to {year}S'] = round(percentage_reduction, 2)\n",
    "\n",
    "# Sort the Year-Term pairs alphabetically, ex2cept for '10F to 10S'\n",
    "reductions = dict(sorted(reductions.items(), key=lambda x2: x2[0] != '10F to 10S'))\n",
    "\n",
    "\n",
    "# Print the percentage reductions\n",
    "for years, reduction in reductions.items():\n",
    "    print(f'{years}: {reduction}% reduction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5854e03",
   "metadata": {},
   "source": [
    "##### Grade Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d9e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#per Dr. Gupta meeting with Dr. Hetrick: remove Computer Information Systems \n",
    "df_grade=df_grade[df_grade['Curr_1_1_Majr_Desc']!='Computer Information Systems']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a281d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the # values in grade cells\n",
    "df_grade['Grde_Code'] = df_grade['Grde_Code'].str.replace('#', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping NaN values\n",
    "df_grade = df_grade.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541bdebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirming all NaN values are dropped\n",
    "df_grade.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb9a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducing the amount of features by combining the first and last name columns to one column\n",
    "\n",
    "# Combine the first and last name columns\n",
    "df_grade['Advr_1_Full_Name'] = df_grade['Advr_1_First_Name'].str.cat(df_grade['Advr_1_Last_Name'], sep=' ')\n",
    "\n",
    "# Drop the first and last name columns\n",
    "df_grade=df_grade.drop(['Advr_1_First_Name', 'Advr_1_Last_Name'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Redundant Features in Grade Data\n",
    "\n",
    "#only 1 value in the column\n",
    "df_grade=df_grade.drop(['Camp_Desc'], axis=1)\n",
    "\n",
    "#Redundant with Curr_1_1_Majr_Desc\n",
    "df_grade=df_grade.drop(['Curr_1_Degc_Desc'], axis=1)\n",
    "\n",
    "#Redundant with Term_Desc\n",
    "df_grade=df_grade.drop(['Term_Code'], axis=1)\n",
    "\n",
    "#Redundant with Study_Year \n",
    "df_grade=df_grade.drop(['Year_Term'], axis=1)\n",
    "\n",
    "#Redundant with Crse_Name + Instructor Name\n",
    "df_grade=df_grade.drop(['CRN'], axis=1)\n",
    "\n",
    "#Redundant with Crse_Name\n",
    "df_grade=df_grade.drop(['Subj_Code'], axis=1)\n",
    "df_grade=df_grade.drop(['Crse_Numb'], axis=1)\n",
    "df_grade=df_grade.drop(['Crse_Title'], axis=1)\n",
    "\n",
    "#Different Teachers can have same section numbers probably will confuse model\n",
    "df_grade=df_grade.drop(['Sect_Numb'], axis=1)\n",
    "\n",
    "#Redundant with already existing GPA information\n",
    "df_grade=df_grade.drop(['Credit_Hr'], axis=1)\n",
    "df_grade=df_grade.drop(['Grde_Code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a77c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information regarding major \n",
    "value_counts = df_grade['Curr_1_1_Majr_Desc'].value_counts()\n",
    "print(value_counts.head(5))\n",
    "print(value_counts.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping duplicate rows\n",
    "df_grade.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09079f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final look at grade data after intial data cleaning\n",
    "df_grade.reset_index(drop=True, inplace=True)\n",
    "print(df_grade.shape)\n",
    "df_grade.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad7800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#per meeting with Dr. Hetrick, find top 5 most common classes for students by year\n",
    "df_grade.Study_Year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdfdd34",
   "metadata": {},
   "source": [
    "Will opt to only do this for the first five years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data for first 4 study years\n",
    "df_grade_1234 = df_grade[df_grade['Study_Year'].isin([1, 2, 3, 4, 5])]\n",
    "\n",
    "# get the top 5 most common Crse_Name for each year\n",
    "top_5 = df_grade_1234.groupby(['Study_Year', 'Crse_Name'])['Crse_Name'].count().reset_index(name='count').sort_values(['Study_Year', 'count'], ascending=[True, False]).groupby('Study_Year').head(5)\n",
    "\n",
    "#Visual for the classes\n",
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new data frame to fill the data in\n",
    "top_5_grades = pd.DataFrame({'PIDM': df_grade['PIDM'].unique()})\n",
    "#Creating columns for each of the year and class combination\n",
    "for i, row in top_5.iterrows():\n",
    "    col_name = 'Study_Year ' + str(row['Study_Year']) + ' Crse_Name ' + row['Crse_Name']\n",
    "    top_5_grades[col_name] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65bed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populating the data\n",
    "#side note takes ~5mins to run\n",
    "for i, row in top_5_grades.iterrows():\n",
    "    pidm = row['PIDM']\n",
    "    # Loop through columns of top_5_grades\n",
    "    for col in top_5_grades.columns[1:]:\n",
    "        study_year = col.split(' ')[1]\n",
    "        crse_name = col.split(' ')[3]\n",
    "        # Find matching row in df_grade\n",
    "        match = df_grade[(df_grade['PIDM'] == pidm) & (df_grade['Study_Year'] == int(study_year)) & (df_grade['Crse_Name'] == crse_name)]\n",
    "        if len(match) > 0:\n",
    "            top_5_grades.loc[i, col] = match.iloc[0]['Grde_Code_Qlty_Pnts']\n",
    "        else:\n",
    "            top_5_grades.loc[i, col] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the new columns that will be added to the final data set\n",
    "top_5_grades.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea951ad",
   "metadata": {},
   "source": [
    "#### Address Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8613d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intial view of the data\n",
    "df_address.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the amount of unique values in each feature of the data\n",
    "df_address.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the amount of NaN and type of data \n",
    "df_address.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6c7c8b",
   "metadata": {},
   "source": [
    "Since PR_NATN_CODE and PR_NATN_DESC have so many nulls, so durring the Data Cleaning the data will be filled for PR_NATN_DESC. PR_NATN_CODE does not tell me anything meanigful, so it will be removed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1587ce90",
   "metadata": {},
   "source": [
    "#### Address Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb90e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Zip code with only first 5 digits\n",
    "\n",
    "# Define a regular expression pattern to match the first five digits\n",
    "pattern = r'^(\\d{5})'\n",
    "\n",
    "# Apply the pattern to the PR_ZIP column using the str.extract() method\n",
    "df_address['ZIP'] = df_address['PR_ZIP'].str.extract(pattern)\n",
    "\n",
    "df_address.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e91799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droping data\n",
    "\n",
    "#Redundant with PR_STAT_CODE\n",
    "df_address=df_address.drop('PR_STAT_DESC', axis=1)\n",
    "\n",
    "#Redundant with ZIP\n",
    "df_address=df_address.drop(['PR_ZIP'], axis=1)\n",
    "\n",
    "\n",
    "#Redundant with PR_NATN_DESC\n",
    "df_address=df_address.drop(['PR_NATN_CODE'], axis=1)\n",
    "\n",
    "df_address.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cacc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving a copy to add back to, allowing for median salary to be given to the values that do not have a ZIP value\n",
    "df_address2=df_address\n",
    "\n",
    "#Removing all NaN in ZIP\n",
    "df_address = df_address.dropna(subset=['ZIP'])\n",
    "df_address.info()\n",
    "\n",
    "#Since I want to find the average household income, the data with no ZIP is removed\n",
    "df_address = df_address.reset_index(drop=True)\n",
    "\n",
    "df_address.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847cb264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since there was some U.S. states that were not didn't have a PR_NATN_DESC, I filled all them as UNITED STATES\n",
    "us_states = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "df_address_us_states = df_address[df_address['PR_STAT_CODE'].isin(us_states)]\n",
    "\n",
    "df_address.loc[df_address_us_states.index, 'PR_NATN_DESC'] = 'UNITED STATES'\n",
    "\n",
    "df_address.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940fdb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning PR_NATN_DESC nulls\n",
    "nan_rows = df_address[df_address['PR_NATN_DESC'].isna()]\n",
    "nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I manually looked the locations up\n",
    "df_address.loc[690, 'PR_NATN_DESC'] = 'UNITED STATES'\n",
    "df_address.loc[1461, 'PR_NATN_DESC'] = 'UNITED STATES'\n",
    "df_address.loc[1601, 'PR_NATN_DESC'] = 'UNITED STATES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking again on NaNs\n",
    "df_address.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aacd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking on the rows with no PR_STAT_CODE\n",
    "nan_rows = df_address[df_address['PR_STAT_CODE'].isna()]\n",
    "nan_rows.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking on the quantity the NaNs are from foreign countries\n",
    "missing_states = df_address[df_address['PR_STAT_CODE'].isna()]\n",
    "natn_desc_counts = missing_states['PR_NATN_DESC'].value_counts()\n",
    "print(natn_desc_counts)\n",
    "sum(natn_desc_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e35d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the words No State for the ones that have no stat\n",
    "df_address[df_address['PR_STAT_CODE'].isna()] = df_address[df_address['PR_STAT_CODE'].isna()].fillna(\"No State\")\n",
    "\n",
    "df_address.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ca38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#US zip code only has data on US, so U.S. will be subsetted\n",
    "df_address_US=df_address[df_address['PR_NATN_DESC']=='UNITED STATES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the median household income through zip code information\n",
    "\n",
    "# Create a SearchEngine instance\n",
    "search = SearchEngine()\n",
    "\n",
    "\n",
    "# Define a function to get the median household income for a ZIP code\n",
    "def get_median_income(zipcode):\n",
    "    try:\n",
    "        return search.by_zipcode(zipcode).median_household_income\n",
    "    except:\n",
    "        return 'N/A'\n",
    "\n",
    "# Runing the function\n",
    "df_address_US.loc[:, 'median_household_income'] = df_address_US['ZIP'].apply(get_median_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ee589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking to see if the median household income had any issues\n",
    "df_address_US.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e7bce",
   "metadata": {},
   "source": [
    "There are unaccounted for null values, I will drop these values and fill them up with the median income of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop null values\n",
    "df_address_US=df_address_US.dropna()\n",
    "#making sure they are dropped\n",
    "df_address_US.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59554700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating median filler for all the nulls\n",
    "median_income = df_address_US['median_household_income'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e75601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the data frames\n",
    "selected_cols = df_address_US[['PIDM', 'median_household_income']]\n",
    "merged_df = df_address2.merge(selected_cols, on='PIDM', how='left')\n",
    "merged_df\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f92244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values in the 'median_household_income' column with 'median_income'\n",
    "merged_df['median_household_income'].fillna(median_income, inplace=True)\n",
    "df_address=merged_df\n",
    "df_address.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83018389",
   "metadata": {},
   "source": [
    "#### Retention Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the first 3 rows of the data\n",
    "df_retention.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking unique values per feature\n",
    "df_retention.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57785f1",
   "metadata": {},
   "source": [
    "The features, DISCIPLINE, college, retrn_cohort and grad_cohort only have 1 value, so they will be removed durring the data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30adb735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the various majors\n",
    "value_counts = df_retention['major'].value_counts()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a9c92",
   "metadata": {},
   "source": [
    "Majors are not the same as the degrees majors in Grades data set, there are also alot less majors in this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking data types and null values counts\n",
    "df_retention.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fda579",
   "metadata": {},
   "source": [
    "I see that there will be NaN that will be cleaned in the data cleaning section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the various regions\n",
    "value_counts = df_retention['region'].value_counts()\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825c923",
   "metadata": {},
   "source": [
    "Will be removing International and Missing because neither of them will be able to be searched in the U.S. zipcode library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434de0f4",
   "metadata": {},
   "source": [
    "#### Retention Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d819e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns\n",
    "\n",
    "#only 1 value columns\n",
    "df_retention = df_retention.drop(['DISCIPLINE', 'college', 'retrn_cohort', 'grad_cohort'], axis=1)\n",
    "\n",
    "#Redundant with  HSGPA_cat\n",
    "df_retention = df_retention.drop(['HSGPA'], axis=1)\n",
    "\n",
    "#Redundant with major\n",
    "df_retention = df_retention.drop(['degc_code'], axis=1)\n",
    "\n",
    "#viewing data again\n",
    "df_retention.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448892a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a column to see if someone graduated or didn't 1:Graduated 0:Didn't Graduate\n",
    "df_retention['graduated'] = df_retention[['grad_3yr', 'grad_4yr', 'grad_5yr', 'grad_6yr']].any(axis=1).astype(int)\n",
    "df_retention.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2015 and before do not have NaN problem \n",
    "#2016 and after has NaN problem due to data only reaching 2020, don't know grad_6yr and before NaN\n",
    "#ex: 2016: grad_6yr=NaN, \n",
    "#    2017: retrn_6yr=Nan grad_5yr&grad_6yr=NaN, \n",
    "#    2018: retrn_5yr&retrn_6yr=Nan grad_4yr&grad_5yr&grad_6yr=NaN, etc\n",
    "\n",
    "#Ex: of 2016\n",
    "df_retention[df_retention['term_code_key']==201681].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db789ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex: of 2020\n",
    "df_retention[df_retention['term_code_key']==202081].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution for now subset out the 2016 and after data\n",
    "df_retention.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking on the years\n",
    "years=df_retention['term_code_key'].unique()\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PG added\n",
    "#x = df_retention['term_code_key'].astype(str).str[:4]\n",
    "df_retention = df_retention[df_retention['term_code_key'].astype(str).str[:4]<'2016']\n",
    "df_retention.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsetting data for 2015 and before\n",
    "years_2015_and_before=years[:11]\n",
    "df_retention2015 = df_retention[df_retention['term_code_key'].isin(years_2015_and_before)]\n",
    "df_retention2015.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76cc0c",
   "metadata": {},
   "source": [
    "Large subset of data is lost due to removal of students from years 2016 and onward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking on how balanced the data is\n",
    "df_retention2015.graduated.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f460b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA of cleanned data\n",
    "#profile = ProfileReport(df_retention2015, title=\"Report\")\n",
    "#profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c098e",
   "metadata": {},
   "source": [
    "#### Joining the Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating columns Crse_Name,Inst_GPA, Curr_1_1_Majr_Desc\n",
    "df_grade_groupby=df_grade.groupby(\"PIDM\",as_index=False, sort = False).aggregate({'Crse_Name':'size', \"Inst_GPA\":'mean','Curr_1_1_Majr_Desc':'nunique'})\n",
    "\n",
    "df_grade_groupby=df_grade_groupby.rename(columns={'Crse_Name': \"Num_of_Crses\",\"Inst_GPA\": \"Mean_GPA\",'Curr_1_1_Majr_Desc': \"Num_of_Unique_Majors\"})\n",
    "df_retention_plus_df_grade = pd.merge(df_grade_groupby,df_retention2015, on='PIDM', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining the USA address data to the rest of the data\n",
    "df_address_income=df_address[['PIDM', 'median_household_income']]\n",
    "df_combined = pd.merge(df_address_income,df_retention_plus_df_grade, on='PIDM', how = 'inner')\n",
    "\n",
    "df_address_income_AE=df_address[['PIDM', 'median_household_income', 'ZIP']]\n",
    "df_combined_AE = pd.merge(df_address_income_AE,df_retention_plus_df_grade, on='PIDM', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0bdf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining the combined data with the top 5 classes in each semester\n",
    "df_combined = pd.merge(df_combined,top_5_grades, on='PIDM', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d4593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing combined data\n",
    "df_combined.head(3)\n",
    "#drop term_code_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking on the shape of the data set\n",
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f194f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking again on the data\n",
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551de304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making all the data types consistent for retrn years\n",
    "df_combined['retrn_2yr'] = df_combined['retrn_2yr'].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b884260",
   "metadata": {},
   "source": [
    "#### EDA of Combined Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e92df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df_combined.select_dtypes('object')\n",
    "df_num = df_combined.select_dtypes(exclude = [object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acce2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_no_crse_name = df_num[['median_household_income', 'Num_of_Crses', 'Mean_GPA',\n",
    "       'Num_of_Unique_Majors', 'term_code_key', 'retrn_2yr', 'retrn_3yr',\n",
    "       'retrn_4yr', 'retrn_5yr', 'retrn_6yr','graduated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3730337",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrn_data = df_combined[['retrn_2yr','retrn_3yr', 'retrn_4yr', 'retrn_5yr', 'retrn_6yr']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577376de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in retrn_data.columns:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.countplot(retrn_data[i], palette = 'hls')\n",
    "    plt.title(\"Distribution By Return Year\")\n",
    "    #sns.countplot(df_retrn_data[i], data = , palette = 'hls')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe7a90",
   "metadata": {},
   "source": [
    "Each year the number of students who return decreases, which makes sense because they are graudating after 4 years usually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f60e646",
   "metadata": {},
   "source": [
    "#### Slide 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ae9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.countplot(x='User', data=df)\n",
    "\n",
    "ax = sns.countplot(x = df_combined['graduated'])\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.title(\"Graduated College vs Did Not Graduate College Count\", fontsize = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2172bc2",
   "metadata": {},
   "source": [
    "Almost 70% of students graduate from college."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a44683",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_short = df_combined[['median_household_income', 'Num_of_Crses', 'Mean_GPA',\n",
    "       'Num_of_Unique_Majors', 'term_code_key', 'HSGPA_cat', 'major',\n",
    "       'ethnicity', 'gender', 'region', 'retrn_2yr', 'retrn_3yr', 'retrn_4yr',\n",
    "       'retrn_5yr', 'retrn_6yr', 'grad_3yr', 'grad_4yr', 'grad_5yr',\n",
    "       'grad_6yr', 'graduated']]\n",
    "\n",
    "df_combined_short_Wo_Grad = df_combined[['median_household_income', 'Num_of_Crses', 'Mean_GPA',\n",
    "       'Num_of_Unique_Majors', 'term_code_key', 'HSGPA_cat', 'major',\n",
    "       'ethnicity', 'gender', 'region', 'retrn_2yr', 'retrn_3yr', 'retrn_4yr',\n",
    "       'retrn_5yr', 'retrn_6yr', 'graduated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd4b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PIDM = pd.read_csv(\"PDIM_Combined_df.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f769695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to Subset all Graduated Students\n",
    "df_combined2 = df_combined\n",
    "\n",
    "all_graduated_students_df = df_combined2[(df_combined2[\"graduated\"] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to Subset all Non-Graduated Students\n",
    "\n",
    "all_non_grad_df = df_combined2[(df_combined2[\"graduated\"] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Slide 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacked bar chart to compare genders and grad/non grad counts\n",
    "\n",
    "females_df = df_combined2[(df_combined2[\"gender\"] == \"F\")]\n",
    "males_df = df_combined2[(df_combined2[\"gender\"] == \"M\")]\n",
    "\n",
    "\n",
    "male_graduates = males_df.loc[males_df['graduated'] == 1, 'gender'].count()\n",
    "male_non_graduates = males_df.loc[males_df['graduated'] == 0, 'gender'].count()\n",
    "female_graduates = females_df.loc[females_df['graduated'] == 1, 'gender'].count()\n",
    "female_non_graduates = females_df.loc[females_df['graduated'] == 0, 'gender'].count()\n",
    "\n",
    "# Create the stacked bar chart\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Set the x-axis tick locations and labels\n",
    "x_ticks = [0, 1]\n",
    "x_labels = ['Non Graduates', 'Graduates']\n",
    "\n",
    "\n",
    "# # Create the bars for males and females\n",
    "# male_bars = ax.bar(x_ticks, [male_non_graduates, male_graduates], label='Males', color='darkblue') # change the value of color in order to change the color of males legend\n",
    "# female_bars = ax.bar(x_ticks, [female_non_graduates, female_graduates], bottom=[male_non_graduates, male_graduates],\n",
    "#        label='Females', color='yellow') # change the value of color in order to change the color of females legend\n",
    "\n",
    "#Create the bars for males and females\n",
    "female_bars = ax.bar(x_ticks, [female_non_graduates, female_graduates], label='Females', color='#C0C0C0') # change the value of color in order to change the color of males legend\n",
    "male_bars = ax.bar(x_ticks, [male_non_graduates, male_graduates], bottom=[female_non_graduates, female_graduates],\n",
    "       label='Males', color='#233C9B') # change the value of color in order to change the color of females legend\n",
    "\n",
    "# Set the x-axis label, y-axis label, and chart title\n",
    "ax.set_xlabel('Graduation')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Graduation Counts by Gender')\n",
    "\n",
    "# Set the x-axis tick locations and labels\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_labels)\n",
    "\n",
    "# Add the legend\n",
    "ax.legend()\n",
    "\n",
    "for i, rect in enumerate(ax.patches):\n",
    "    # Find where everything is located\n",
    "    height = rect.get_height()\n",
    "    width = rect.get_width()\n",
    "    x = rect.get_x()\n",
    "    y = rect.get_y()\n",
    "\n",
    "    # The height of the bar is the count value and can used as the label\n",
    "    label_text = f'{height:.0f}'\n",
    "\n",
    "    label_x = x + width / 2\n",
    "    label_y = y + height / 2\n",
    "\n",
    "    # don't include label if it's equivalently 0\n",
    "    if height > 0.001:\n",
    "        ax.text(label_x, label_y, label_text, ha='center', va='center', color='white', fontsize=12, fontweight='bold')\n",
    "        \n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed3147",
   "metadata": {},
   "source": [
    "#### Slide 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bdcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding count of student ethnicities\n",
    "# Pie Chart Created in Excel\n",
    "# Note: Gray pie chart was created using data from US Census Bureau Website (Referenced in Slides)\n",
    "# U.S. Census Bureau QuickFacts: Stockton city, California. (n.d.). Www.census.gov. https://www.census.gov/quickfacts/stocktoncitycalifornia \n",
    "\n",
    "print(df_combined2[\"ethnicity\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d11315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Slides 17, 18, 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Slide 17\n",
    "\n",
    "# BiDirectional Chart that Shows Ethnicites of Grads and Nons (ALL STUDENTS)\n",
    "\n",
    "grad_data1 = df_combined2[df_combined2['graduated'] == 1]\n",
    "non_grad_data1 = df_combined2[df_combined2['graduated'] == 0]\n",
    "\n",
    "\n",
    "grad_counts = list(grad_data1.groupby('ethnicity')['graduated'].count())\n",
    "non_grad_counts = list(non_grad_data1.groupby('ethnicity')['graduated'].count())\n",
    "\n",
    "non_grad_counts = [x*-1 for x in non_grad_counts]\n",
    "\n",
    "ethnicities = list(df_combined2['ethnicity'].unique())\n",
    "\n",
    "# Create a figure and axis object\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "width = 0.6\n",
    "\n",
    "ax.barh(ethnicities, grad_counts, height=width)\n",
    "ax.barh(ethnicities, non_grad_counts, height=width)\n",
    "\n",
    "# Set the position of the center axis and hide the spines\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Number of Individuals')\n",
    "ax.set_ylabel('Ethnicity')\n",
    "ax.set_title('Ethnicities of All College Graduates vs. Non-Graduates')\n",
    "\n",
    "plt.legend(['','Graduate','Non-Graduate'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a4c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slide 17\n",
    "\n",
    "\n",
    "all_ethnicities_pie_chart = df_combined2\n",
    "\n",
    "print(all_ethnicities_pie_chart['ethnicity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16af9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Slide 18\n",
    "\n",
    "# BiDirectional Chart that Shows Ethnicites of Grads and Nons (MALES)\n",
    "\n",
    "df2_bi_charts =df_combined2\n",
    "\n",
    "\n",
    "male_df = df2_bi_charts[df2_bi_charts['gender'] == 1]\n",
    "male_df = df2_bi_charts[df2_bi_charts['gender'] == 1]\n",
    "grad_data = df2_bi_charts[df2_bi_charts['graduated'] == 1]\n",
    "non_grad_data = df2_bi_charts[df2_bi_charts['graduated'] == 0]\n",
    "\n",
    "\n",
    "grad_counts = list(grad_data.groupby('ethnicity')['graduated'].count())\n",
    "non_grad_counts = list(non_grad_data.groupby('ethnicity')['graduated'].count())\n",
    "\n",
    "non_grad_counts = [x*-1 for x in non_grad_counts]\n",
    "\n",
    "ethnicities = list(df_PIDM['ethnicity'].unique())\n",
    "ethnicities.sort()\n",
    "\n",
    "# Create a figure and axis object\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "width = 0.5\n",
    "\n",
    "ax.barh(ethnicities, grad_counts, height=width)\n",
    "ax.barh(ethnicities, non_grad_counts, height=width)\n",
    "\n",
    "# Set the position of the center axis and hide the spines\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Student Count')\n",
    "ax.set_ylabel('Ethnicity')\n",
    "ax.set_title('Ethnicities of Male College Graduates vs. Non-Graduates')\n",
    "\n",
    "plt.legend(['','Graduate','Non-Graduate'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slide 18\n",
    "\n",
    "# What are the ethnicities of students who did graduated?\n",
    "\n",
    "all_grad_ethnicity_percentage = all_graduated_students_df['ethnicity'].value_counts()\n",
    "all_grad_ethnicity_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbdedbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Slide 18\n",
    "\n",
    "# BiDirectional Chart that Shows Ethnicites of Grads and Nons (MALES)\n",
    "\n",
    "df2_bi_charts =df_combined2\n",
    "\n",
    "\n",
    "male_df = df2_bi_charts[df2_bi_charts['gender'] == 1]\n",
    "male_df = df2_bi_charts[df2_bi_charts['gender'] == 1]\n",
    "grad_data = df2_bi_charts[df2_bi_charts['graduated'] == 1]\n",
    "non_grad_data = df2_bi_charts[df2_bi_charts['graduated'] == 0]\n",
    "\n",
    "\n",
    "grad_counts = list(grad_data.groupby('ethnicity')['graduated'].count())\n",
    "non_grad_counts = list(non_grad_data.groupby('ethnicity')['graduated'].count())\n",
    "\n",
    "non_grad_counts = [x*-1 for x in non_grad_counts]\n",
    "\n",
    "ethnicities = list(df_PIDM['ethnicity'].unique())\n",
    "ethnicities.sort()\n",
    "\n",
    "# Create a figure and axis object\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "width = 0.5\n",
    "\n",
    "ax.barh(ethnicities, grad_counts, height=width)\n",
    "ax.barh(ethnicities, non_grad_counts, height=width)\n",
    "\n",
    "# Set the position of the center axis and hide the spines\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Student Count')\n",
    "ax.set_ylabel('Ethnicity')\n",
    "ax.set_title('Ethnicities of Male College Graduates vs. Non-Graduates')\n",
    "\n",
    "plt.legend(['','Graduate','Non-Graduate'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slide 19\n",
    "\n",
    "# What are the ethnicities of students who did NOT graduate?\n",
    "\n",
    "all_nongrad_ethnicity_percentage = all_non_grad_df['ethnicity'].value_counts()\n",
    "all_nongrad_ethnicity_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Slide 19\n",
    "\n",
    "# BiDirectional Chart that Shows Ethnicites of Grads and Nons (FEMALES)\n",
    "\n",
    "df2_bi_charts =df_combined2\n",
    "\n",
    "\n",
    "female_df = df2_bi_charts[df2_bi_charts['gender'] == 1]\n",
    "female_df = df2_bi_charts[df2_bi_charts['gender'] == 1]\n",
    "fgrad_data = df2_bi_charts[df2_bi_charts['graduated'] == 1]\n",
    "fnon_grad_data = df2_bi_charts[df2_bi_charts['graduated'] == 0]\n",
    "\n",
    "\n",
    "fgrad_counts = list(fgrad_data.groupby('ethnicity')['graduated'].count())\n",
    "fnon_grad_counts = list(fnon_grad_data.groupby('ethnicity')['graduated'].count())\n",
    "\n",
    "fnon_grad_counts = [x*-1 for x in fnon_grad_counts]\n",
    "\n",
    "ethnicities = list(df_PIDM['ethnicity'].unique())\n",
    "ethnicities.sort()\n",
    "\n",
    "# Create a figure and axis object\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "width = 0.5\n",
    "\n",
    "ax.barh(ethnicities, fgrad_counts, height=width)\n",
    "ax.barh(ethnicities, fnon_grad_counts, height=width)\n",
    "\n",
    "# Set the position of the center axis and hide the spines\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Student Count')\n",
    "ax.set_ylabel('Ethnicity')\n",
    "ax.set_title('Ethnicities of Male College Graduates vs. Non-Graduates')\n",
    "\n",
    "plt.legend(['','Graduate','Non-Graduate'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a660790",
   "metadata": {},
   "outputs": [],
   "source": [
    "####  Slides 15 & 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8333338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slide 15\n",
    "# Data for green pie chart (Excel)\n",
    "# Finding majors of non grad males\n",
    "\n",
    "male_majors = df_combined2[df_combined2['gender'] == 1] \n",
    "male_majors_NG = male_majors[male_majors['major'] == 0] \n",
    "\n",
    "print(male_majors_NG['major'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slide 15\n",
    "# Data for gold pie chart (Excel)\n",
    "# Finding majors of non grad males\n",
    "\n",
    "male_majors = df_combined2[df_combined2['gender'] == 1] \n",
    "male_majors_G = male_majors[male_majors['major'] == 1] \n",
    "\n",
    "print(male_majors_G['major'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe3cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slide 16\n",
    "# Data for blue pie chart (Excel)\n",
    "# Finding majors of non grad female \n",
    "\n",
    "female_majors = df_combined2[df_combined2['gender'] == 0] \n",
    "female_majors_NG = female_majors[female_majors['major'] == 0] \n",
    "\n",
    "print(female_majors_NG['major'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ff8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slide 16\n",
    "# Data for orange pie chart (Excel)\n",
    "# Finding majors of non grad female \n",
    "\n",
    "female_majors = df_combined2[df_combined2['gender'] == 0] \n",
    "female_majors_G = female_majors[female_majors['major'] == 1] \n",
    "\n",
    "print(female_majors_G['major'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding majors of all students who did NOT graduate\n",
    "\n",
    "majors_all_graduated_students_df = df_combined2\n",
    "print(all_non_grad_df['major'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471f5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding majors of all students who did graduate\n",
    "\n",
    "all_graduated_students_df = df_combined2\n",
    "print(all_graduated_students_df['major'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Slide 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5911e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# International Students Data\n",
    "\n",
    "int_students2 = df_PIDM[df_PIDM['ethnicity'] == '1. International']\n",
    "int_students2 = int_students2[int_students2['graduated'] == 0]\n",
    "int_students2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c40b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_students2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int_students2['region'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51096a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int_students2['Study_Year 1 Crse_Name PACS1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe7f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int_students2['Mean_GPA'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f82fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int_students2['Num_of_Crses'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7309f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int_students2['major'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea195a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d39b9edc",
   "metadata": {},
   "source": [
    "#### Slide 28 Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The default cmap is sns.cm.rocket. To reverse it set cmap to sns.cm.rocket_r\n",
    "#Creates a correlation Heatmap of our variables\n",
    "\n",
    "#Bigger Figure Size\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "cmap = sns.cm.rocket_r\n",
    "heatmap = sns.heatmap(df_combined.corr(), cmap = cmap)\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407025b2",
   "metadata": {},
   "source": [
    "The correlation coefficients with the variable \"graduated.\" Based on the coefficients provided, it appears that the variables \"retrn_2yr,\" \"retrn_5yr,\" \"grad_4yr,\" have positive correlations with \"graduated.\" This suggests that as these variables increase, there is a tendency for the \"graduated\" variable to increase as well. On the other hand, the variables \"PIDM,\" has a negative correlation with \"graduated,\" which indicates that as this variable increases, there is a tendency for the \"graduated\" variable to decrease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c37932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the correlation that the \"Mean_GPA\" column has with the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.corrwith(df_combined['Mean_GPA']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af82b43",
   "metadata": {},
   "source": [
    "Mean_GPA and the columns that signify retetion have a positive correlation.\n",
    "Mean_GPA and graduated have a correlation of 0.63, Mean_GPA and retrn_2yr have a correlation of 0.44, Mean_GPA and retrn_3yr have a correlation of 0.55, Mean_GPA and retrn_4yr have a correlation of 0.56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19688e52",
   "metadata": {},
   "source": [
    "#### Slide 28 Ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3fb906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a countplot of graduated vs not graduated by gender\n",
    "ax = sns.countplot(x = 'graduated',hue = 'gender', data = df_combined)\n",
    "plt.title(\"Graduated From College vs Not Graduated From College by Gender\", fontsize = 20)\n",
    "plt.legend(['Male', 'Female']);\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c88164",
   "metadata": {},
   "source": [
    "Approximately 76% of females graduated from college compared to approximately 68% of males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizes each major and counts how many graduated vs did not\n",
    "plt.figure(figsize=(16, 8)) \n",
    "ax = sns.countplot(x = 'graduated',hue = 'major', data = df_combined)\n",
    "plt.title(\"College Graduation by Major\", fontsize = 20);\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734218c",
   "metadata": {},
   "source": [
    "Mechanical Engineering had the highest amount of students who did not graduate from college, then Civil Engineering students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ad9b7",
   "metadata": {},
   "source": [
    "#### Slide 26 Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082818aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizes how many people graduated and did not by region\n",
    "plt.figure(figsize=(12, 8)) \n",
    "sns.set_palette(\"pastel\")\n",
    "ax = sns.countplot(x = 'region',hue = 'graduated', data = df_combined)\n",
    "#plt.title(\"College Graduation by Region\", fontsize = 20)\n",
    "\n",
    "plt.legend(['Did Not Graduate', 'Graduated']);\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e78f5a",
   "metadata": {},
   "source": [
    "Highest population of students comes from Northern CA or Local Market. Approximately 73% of Local Market students graduated from college and 74% of Northern CA graduated from college. International had more students who did not grsduate from college."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d08b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Dataframes\n",
    "#Creates a data frame of just male students.\n",
    "df_male = df_combined_AE.loc[df_combined_AE['gender'] == \"M\"]\n",
    "\n",
    "#Creates a data frame of just female students.\n",
    "df_female = df_combined_AE.loc[df_combined_AE['gender'] == \"F\"]\n",
    "\n",
    "#Creates a data frame of students whose High School GPA was less than 3.10.\n",
    "df_HSGPA_less310 = df_combined_AE.loc[df_combined_AE['HSGPA_cat'] == \"Less 3.10\"]\n",
    "\n",
    "#Creates a data frame of students whose High School GPA was between 3.10 and 3.38.\n",
    "df_HSGPA_310_to338 = df_combined_AE.loc[df_combined_AE['HSGPA_cat'] == \"3.10-3.38\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096036e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Dataframes by region\n",
    "#Creates a data frame of students that live in the Western region.\n",
    "df_Western = df_combined_AE.loc[df_combined_AE['region'] == \"Western US\"]\n",
    "\n",
    "#Creates a data frame of students that live in the Eastern region.\n",
    "df_Eastern = df_combined_AE.loc[df_combined_AE['region'] == \"Eastern US\"]\n",
    "\n",
    "#Creates a data frame of students that live in the Local Market region.\n",
    "df_LocalMarket = df_combined_AE.loc[df_combined_AE['region'] == \"Local Market\"]\n",
    "\n",
    "#Creates a data frame of students that live in in the Northern California region.\n",
    "df_NorthernCA = df_combined_AE.loc[df_combined_AE['region'] == \"Northern CA\"]\n",
    "\n",
    "#Creates a data frame of students that live in the Southern California region.\n",
    "df_SouthernCA = df_combined_AE.loc[df_combined_AE['region'] == \"Southern CA\"]\n",
    "\n",
    "#Creates a data frame of students that live in in the International region.\n",
    "df_International = df_combined_AE.loc[df_combined_AE['region'] == \"International\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a75de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Western\n",
    "#Finds male and femal counts of students that live in the Western region.\n",
    "df_Western['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e47a50",
   "metadata": {},
   "source": [
    "A large majority of the Western US region students are male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5af512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the various ethnicity counts of students that live in the Western region.\n",
    "df_Western['ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e9229",
   "metadata": {},
   "source": [
    "A large majority of students that live in the Western US region are White or Asian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the various zip code counts of students that live in the Western region.\n",
    "df_Western['ZIP'].value_counts().nlargest(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7d55f",
   "metadata": {},
   "source": [
    "Students in the Western US region come from the zip codes of 96822 and 96734 the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of the students HS GPA category that live in the Western region.\n",
    "df_Western['HSGPA_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a675d0f",
   "metadata": {},
   "source": [
    "About 50% of the students in the Western US region had HS GPA of 3.60 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Eastern\n",
    "#Finds male and female counts of students that live in the Eastern region.\n",
    "df_Eastern['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af73412c",
   "metadata": {},
   "source": [
    "A large majority of students that live in the Eastern US region are male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbfaa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the various ethnicity counts of students that live in the Eastern region.\n",
    "df_Eastern['ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d1bef",
   "metadata": {},
   "source": [
    "A large majority of students that live in the Eastern US region are White."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of the students HS GPA category that live in the Eastern region.\n",
    "df_Eastern['HSGPA_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0936ae68",
   "metadata": {},
   "source": [
    "About 40% of the students from the Eastern US region had HS GPA of 3.60 or higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds male and female counts of students that live in the Local Market region.\n",
    "df_LocalMarket['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c72ae5",
   "metadata": {},
   "source": [
    "A large majority of students that live in the Local Market region are male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f7064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the various ethnicity counts of students that live in the Local Market region.\n",
    "df_LocalMarket['ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af947c47",
   "metadata": {},
   "source": [
    "A large majority of students that live in the Local Market region are White, Hispanic/Latino or Asian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the various zip code counts of students that live in the Local Market region.\n",
    "df_LocalMarket['ZIP'].value_counts().nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd3d77",
   "metadata": {},
   "source": [
    "Most students in the Local Market region comes from the 95206 zip code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of the students HS GPA category that live in the Local Market region.\n",
    "df_LocalMarket['HSGPA_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb89437",
   "metadata": {},
   "source": [
    "About 47% of students in the Local Market had HS GPA of 3.60 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b23c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds male and female counts of students that live in the Northern California region.\n",
    "df_NorthernCA['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa5eb0",
   "metadata": {},
   "source": [
    "A large majority of students that live in the Northern CA region are male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the various ethnicity counts of students that live in the Northern California region.\n",
    "df_NorthernCA['ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92169309",
   "metadata": {},
   "source": [
    "A large majority of students that live in the Northern CA region are White or Asian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e7814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the various zip code counts of students that live in the Northern California region.\n",
    "df_NorthernCA['ZIP'].value_counts().nlargest(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c4f52",
   "metadata": {},
   "source": [
    "A majority of the students who live in the Northern CA region come from the 94539 zip code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefc150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of the students HS GPA category that live in the Northern California region.\n",
    "df_NorthernCA['HSGPA_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831c0b3",
   "metadata": {},
   "source": [
    "About 35.2% of students in Northern CA had HS GPA of 3.60 or higher. About 16.2% of students had HS GPA of less than 3.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d66ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds male and female counts of students that live in the Southern California region.\n",
    "df_SouthernCA['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec12c52",
   "metadata": {},
   "source": [
    "A large majority of students that live in the Southern CA region are male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb203f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the various ethnicity counts of students that live in the Southern California region.\n",
    "df_SouthernCA['ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81cf147",
   "metadata": {},
   "source": [
    "A large majority of students that live in the Southern CA region are White."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of the students HS GPA category that live in the Southern California region.\n",
    "df_SouthernCA['HSGPA_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977df2b",
   "metadata": {},
   "source": [
    "Only about 27% of Students in the Southern CA region had HS GPA over 27%. About 18% had HS GPA of less than 3.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa03fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds male and female counts of students that live in the International region.\n",
    "df_International['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732dabc",
   "metadata": {},
   "source": [
    "A large majority of students that live in the International region are male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the various ethnicity counts of students that live in the International region.\n",
    "df_International['ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b8f4e",
   "metadata": {},
   "source": [
    "If a student lives in the International region, their ethnicity is also International."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of the students HS GPA category that live in the International region.\n",
    "df_International['HSGPA_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9095bf",
   "metadata": {},
   "source": [
    "About 27.5% of students in the International region had HS GPA of 3.60 or higher. About 23.2% of students had HS GPA less than 3.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizes how many people graduated and did not by region of only males.\n",
    "plt.figure(figsize=(12, 8)) \n",
    "sns.set_palette(\"pastel\")\n",
    "ax = sns.countplot(x = 'region',hue = 'graduated', data = df_male)\n",
    "#plt.title(\"Male Graduation Rates by Region\")\n",
    "plt.legend(['Did Not Graduate', 'Graduated']);\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05419e87",
   "metadata": {},
   "source": [
    "Northern CA and Local Market had the highest college graduation rates among males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca23be91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizes how many people graduated and did not by region of only females.\n",
    "plt.figure(figsize=(12, 8)) \n",
    "sns.set_palette(\"pastel\")\n",
    "ax = sns.countplot(x = 'region',hue = 'graduated', data = df_female)\n",
    "plt.title(\"Female Graduation Rates by Region\", fontsize = 20)\n",
    "plt.legend(['Did Not Graduate', 'Graduate'])\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f9f7d",
   "metadata": {},
   "source": [
    "Students in the Northern CA and Local Market had the highest college graduation rates among Females as well. The amount of students who live in the regions of Western US and International are high but there sample size is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f883eabc",
   "metadata": {},
   "source": [
    "Females had a higher college graduation rate than males in each region except females that live in the Southern California region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6682ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizes how many people graduated and did not by region of only students with a High School GPA under 3.10.\n",
    "plt.figure(figsize=(12, 8)) \n",
    "sns.set_palette(\"pastel\")\n",
    "ax = sns.countplot(x = 'region',hue = 'graduated', data = df_HSGPA_less310)\n",
    "plt.title(\"College Graduation by Region for Students with a High School GPA under 3.10\", fontsize = 20)\n",
    "plt.legend(['Did Not Graduate', 'Graduated'], loc='upper right')\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1e66d",
   "metadata": {},
   "source": [
    "Students with HS GPA less than 3.10 struggle in college. Students in the Local Market and Northern California regions have the highest population and their graduation rate is around 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizes how many people graduated and did not by region of only students with a High School GPA between 3.10 and 3.38.\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set_palette(\"pastel\")\n",
    "ax = sns.countplot(x = 'region',hue = 'graduated', data = df_HSGPA_310_to338)\n",
    "plt.title(\"College Graduation by Region for Students with a High School GPA between 3.10 and 3.38\", fontsize = 20)\n",
    "plt.legend(['Did Not Graduate', 'Graduated'])\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b578f4df",
   "metadata": {},
   "source": [
    "The students whose HS GPA is between 3.10 and 3.38 have a higher college graduation rate than the students whose GPA is less than 3.10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f3620",
   "metadata": {},
   "source": [
    "#### Slide 26 Ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac41789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizes how many people graduated and did not by ethnicity.\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.set_palette(\"pastel\")\n",
    "ax = sns.countplot(x = 'graduated',hue = 'ethnicity', data = df_combined)\n",
    "plt.title(\"College Graduation by Ethnicity\", fontsize = 20);\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c50ff",
   "metadata": {},
   "source": [
    "Every ethnicity category has more students who graduate from college vs did not graduate except for International. The majority of the dataset consists of White, Asian, and Hispanic/Latino.\n",
    "White students have a college graduation rate of 74%, Asian students have a graduation rate of 66%, Hispanic/Latino students have a graduation rate of 64%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4317aa",
   "metadata": {},
   "source": [
    "#### Start of Slide 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77524ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizes how many people graduated and did not by HS GPA category.\n",
    "plt.figure(figsize=(12, 8)) \n",
    "sns.set_palette(\"pastel\")\n",
    "ax = sns.countplot(x = 'HSGPA_cat',hue = 'graduated', data = df_combined)\n",
    "plt.title(\"College Graduation by High School GPA Category\", fontsize = 20)\n",
    "plt.legend(['Did Not Graduate', 'Graduated']);\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65a67a",
   "metadata": {},
   "source": [
    "The higher a students HS GPA category, the more likely they are to graduate from college. Only 52% of students who have a HS GPA of less than 3.10 graduated from college."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a677693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a dataframe of students I deem to have high HS GPA (Above 3.6).\n",
    "df_high_HSGPA=df_combined_AE.loc[(df_combined_AE['HSGPA_cat'] == \"3.60-3.82\") | (df_combined_AE['HSGPA_cat'] == \"3.83-4.00\")]\n",
    "\n",
    "#Creates a dataframe of students I deem to have low HS GPA (3.38 or less).\n",
    "#I do not know the exact HS GPA of the students.\n",
    "#The lowest category is students whose HS GPA is under 3.10.\n",
    "df_low_HSGPA = df_combined_AE.loc[(df_combined_AE['HSGPA_cat'] == \"Less 3.10\") | (df_combined_AE['HSGPA_cat'] == \"3.10-3.38\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of students with high HS GPA by region.\n",
    "HighHSGPARegions = df_high_HSGPA['region'].value_counts().nlargest(5)\n",
    "HighHSGPARegions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of students with low HS GPA by region.\n",
    "LowHSGPARegions = df_low_HSGPA['region'].value_counts().nlargest(5)\n",
    "LowHSGPARegions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67aa380",
   "metadata": {},
   "source": [
    "The smartest students came from the Local Market and Northern CA regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots a piechart of with high HS GPA by region.\n",
    "labels = 'Local Market', 'Northern CA', 'Western US', 'Southern CA',  'International'\n",
    "plt.pie(HighHSGPARegions, labels=labels, autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots a piechart of with low HS GPA by region.\n",
    "labels =  'Northern CA', 'Local Market', 'Southern CA', 'Western US',   'International'\n",
    "plt.pie(LowHSGPARegions, labels=labels, autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c9936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of students with high HS GPA by major.\n",
    "HighHSGPAMajors = df_high_HSGPA['major'].value_counts().nlargest(5)\n",
    "HighHSGPAMajors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a18564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of students with low HS GPA by major.\n",
    "LowHSGPAMajors= df_low_HSGPA['major'].value_counts().nlargest(5)\n",
    "LowHSGPAMajors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a94980",
   "metadata": {},
   "source": [
    "The proportion of students with high vs low HS GPA in each major is around 50% except for Bioengineering.\n",
    "61% of the Bioengineering students had a high HS GPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aae969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots a piechart of with high HS GPA by major.\n",
    "labels =  'Mechanical Engineering', 'Civil Engineering ', 'Bioengineering', 'Computer Science ',  'Exploratory (Engineering)'\n",
    "plt.pie(HighHSGPAMajors, labels=labels, autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e906272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots a piechart of with low HS GPA by major.\n",
    "labels =  'Mechanical Engineering', 'Civil Engineering ', 'Computer Science ',  'Exploratory (Engineering)', 'Bioengineering',\n",
    "plt.pie(LowHSGPAMajors, labels=labels, autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e5c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of students with high HS GPA by gender.\n",
    "HighHSGPAGender = df_high_HSGPA['gender'].value_counts()\n",
    "HighHSGPAGender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e12c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of students with low HS GPA by gender.\n",
    "LowHSGPAGender = df_low_HSGPA['gender'].value_counts()\n",
    "LowHSGPAGender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f6d7b",
   "metadata": {},
   "source": [
    "There were more males with a low HS GPA than males with a high HS GPA.\n",
    "There were more females with a high HS GPA than females with a low HS GPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots a piechart of with high HS GPA by gender.\n",
    "labels =  \"Male\", \"Female\"\n",
    "\n",
    "plt.pie(HighHSGPAGender, labels=labels, autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots a piechart of with low HS GPA by gender.\n",
    "labels =  \"Male\", \"Female\"\n",
    "plt.pie(LowHSGPAGender, labels=labels, autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c155992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of students with high HS GPA by ethnicity.\n",
    "HighHSGPAEthnicity = df_high_HSGPA['ethnicity'].value_counts().nlargest(5)\n",
    "HighHSGPAEthnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of students with low HS GPA by ethnicity.\n",
    "LowHSGPAEthnicity = df_low_HSGPA['ethnicity'].value_counts().nlargest(5)\n",
    "LowHSGPAEthnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db4c39",
   "metadata": {},
   "source": [
    "White people had more than 50% of its students with a high HS GPA. \n",
    "All the other ethnicities had around 50% of its students with a high HS GPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce3228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots a piechart of with high HS GPA by ethnicity.\n",
    "labels =  \"White\", \"Asian\", 'Hispanic/Latino', 'Two or more races', 'Race and ethnicity unknown'\n",
    "plt.pie(HighHSGPAEthnicity, labels=labels, autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots a piechart of with low HS GPA by ethnicity.\n",
    "labels =  'White', 'Asian', 'Hispanic/Latino', 'International','Race and ethnicity unknown'\n",
    "plt.pie(LowHSGPAEthnicity, labels=labels, autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcdfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the count of students with high HS GPA by zip code.\n",
    "df_high_HSGPA['ZIP'].value_counts().nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a7e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "##inds the count of students with low HS GPA by zip code.\n",
    "df_low_HSGPA['ZIP'].value_counts().nlargest(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df450027",
   "metadata": {},
   "source": [
    "The smartest students came from 95206 zip code. This zip code area covers the cities of Stockton, Taft, Mosswood, Gillis, and Holt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cda098",
   "metadata": {},
   "source": [
    "#### End of Slide 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa94f6a",
   "metadata": {},
   "source": [
    "#### Start of Slide 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots a histogram of the mean cumulative college GPA of students.\n",
    "# create 10 bins between 0 and 4 with equal width\n",
    "bins = np.linspace(0, 4, 11)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# get the labels for the bins\n",
    "bin_labels = [f\"{bins[i]:.1f}-{bins[i+1]:.1f}\" for i in range(len(bins)-1)]\n",
    "# cut the GPA values into the bins\n",
    "bin_counts = pd.cut(df_combined['Mean_GPA'], bins=bins, labels=bin_labels, include_lowest=True).value_counts(sort=False)\n",
    "# create a bar graph of the bin counts with increased width\n",
    "plt.bar(bin_counts.index.astype(str), bin_counts.values, width=0.8)\n",
    "# set x-axis label\n",
    "plt.xlabel('Cumulative University GPA')\n",
    "# set y-axis label\n",
    "plt.ylabel('Count')\n",
    "#Set graph title\n",
    "plt.title(\"Distribution of Cumulative GPA\")\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13e99e",
   "metadata": {},
   "source": [
    "More than 100 people are below 2.0 cumulative college GPA. Find out what they have in common!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7862bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a dataframe of students who mean cumulative college GPA is under 2.0.\n",
    "under_2_Gpa = df_combined_AE.loc[df_combined_AE['Mean_GPA'] < 2.0]\n",
    "under_2_Gpa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a55a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the number of rows (students) whose cumulative college GPA is under 2.0.\n",
    "\n",
    "under_2_Gpa.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9600f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the dataframe of students under 2.0 GPA who did not graduate.\n",
    "under_2_Not_graduate = under_2_Gpa.loc[under_2_Gpa['graduated'] == 0]\n",
    "\n",
    "under_2_Not_graduate.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54048da",
   "metadata": {},
   "source": [
    "All the 183 students whose cumulative college was under 2.0 GPA did not graduate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2ef5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds out the percentage of students in each major under 2.0 cumulative college GPA against the whole population of students in the dataset.\n",
    "under_2_Gpa['major'].value_counts()/df_combined['major'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dccaa04",
   "metadata": {},
   "source": [
    "For the students whose cumulative college GPA was under 2.0, the highest proportion against the total population of students in the dataset was students in the Engineering Physics major."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a073b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the top 5 most frequent zip codes.\n",
    "under_2_Gpa['ZIP'].value_counts().nlargest(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce4141",
   "metadata": {},
   "source": [
    "For the students whose cumulative college GPA was under 2.0, these are the top 5 zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7166a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds out the count of ethnicities for students whose cumulative college GPA is under 2.0.\n",
    "under_2_Gpa['ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3e813",
   "metadata": {},
   "source": [
    "For the students whose cumulative college GPA was under 2.0, the most common ethnicity was White. Find ratio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e279f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds out the percentage of students in each ethnicity under 2.0 cumulative college GPA against the whole population of students in the dataset.\n",
    "(under_2_Gpa['ethnicity'].value_counts())/(df_combined['ethnicity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d70ca7",
   "metadata": {},
   "source": [
    "More analysis leads us to see that even though White students had the most number of students of under 2.0 cumulative college GPA, the proportion against the total population of White students was about 12.3%. The highest proportion was American Indian or Alaska Native at 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24111fff",
   "metadata": {},
   "source": [
    "##### Slide 25 ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e34d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizes how many people graduated from college and by number of unique majors.\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.set_palette(\"pastel\")\n",
    "ax = sns.countplot(x = 'graduated',hue = 'Num_of_Unique_Majors', data = df_combined)\n",
    "plt.title(\"College Graduation by Number of Unique Majors\", fontsize = 20);\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7744c",
   "metadata": {},
   "source": [
    "A majority of the dataset consists of students who kept their original major throughout their studies. The graduation rate is 63%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c426f",
   "metadata": {},
   "source": [
    "#### Start of Slide 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3906cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gpa's for majors with higher drop out rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a data frame of students who graduated from college.\n",
    "df_graduated = df_combined_AE.loc[df_combined_AE['graduated'] == 1]\n",
    "df_graduated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9161e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the dataframe of students who did not graduate from college.\n",
    "df_Not_Graduate = df_combined.loc[df_combined['graduated'] == 0]\n",
    "df_Not_Graduate.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70785244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the dataframe of students who did not return for year 2 that did not graduate from college.\n",
    "notReturn_2yr = df_Not_Graduate.loc[df_Not_Graduate['retrn_2yr'] == 0]\n",
    "notReturn_2yr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12601b",
   "metadata": {},
   "source": [
    "41% of the students who did not graduate did not return in the second year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80485b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the dataframe of students who did not return for year 3 that did not graduate from college.\n",
    "notReturn_3yr = df_Not_Graduate.loc[df_Not_Graduate['retrn_3yr'] == 0]\n",
    "notReturn_3yr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27a298",
   "metadata": {},
   "source": [
    "Almost 70% of students who did not graduate did not return for their third year. They Dropped out after their 2nd year, and did not come back for their 3rd year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd9a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the dataframe of students who did not return for year 4 that did not graduate from college.\n",
    "notReturn_4yr = df_Not_Graduate.loc[df_Not_Graduate['retrn_4yr'] == 0]\n",
    "notReturn_4yr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the top 16 most common number of courses students took who did not graduate from college.\n",
    "df_Not_Graduate['Num_of_Crses'].value_counts().nlargest(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3c8bf",
   "metadata": {},
   "source": [
    "For the people who did not graduate, the majority of the students only took less than 20 courses before they left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca81c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of students who graduated in each major divided by the umber of students in the major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679804bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graduation_rate_by_major = ((df_graduated['major'].value_counts())/(df_combined['major'].value_counts())).sort_values()\n",
    "\n",
    "graduation_rate_by_major = round(graduation_rate_by_major, 3)\n",
    "print(graduation_rate_by_major)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194865ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of students who graduated in each major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98434d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graduated['major'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of students in each major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb53b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['major'].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b72369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the counts of major choices of students who did not graduate from college.\n",
    "df_Not_Graduate['major'].value_counts().nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005300cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find a dataframe of Mechanical Engineering students who did not graduate from college.\n",
    "df_Not_Graduate_Mech_Eng = df_Not_Graduate.loc[df_combined[\"major\"] == \"Mechanical Engineering\"]\n",
    "\n",
    "##Find a dataframe of Civil Engineering students who did not graduate from college.\n",
    "df_Not_Graduate_Civ_Eng = df_Not_Graduate.loc[df_combined[\"major\"] == \"Civil Engineering\"]\n",
    "\n",
    "##ind a dataframe of Computer Science students who did not graduate from college.\n",
    "df_Not_Graduate_Comp_Sci = df_Not_Graduate.loc[df_combined[\"major\"] == \"Computer Science\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f757afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the number of rows (students) in the dataframe of Mechanical Engineering students who did not graduate.\n",
    "df_Not_Graduate_Mech_Eng.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e2282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds a dataframe of Mechanical Engineering Students under 3.0 cumulatitive college GPA.\n",
    "df_Not_Graduate_Mech_Engunder3Gpa = df_Not_Graduate_Mech_Eng.loc[df_combined[\"Mean_GPA\"] < 3.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the number of rows (students) in the dataframe of Mechanical Engineering Students under 3.0 cumulatitive college GPA.\n",
    "df_Not_Graduate_Mech_Engunder3Gpa.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd600a",
   "metadata": {},
   "source": [
    "For the Mechanical Engineering major, 85% of the students who did not graduate had a mean gpa of under 3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds a dataframe of Mechanical Engineering Students under 2.75 cumulatitive college GPA.\n",
    "df_Not_Graduate_Mech_Engunder275Gpa = df_Not_Graduate_Mech_Eng.loc[df_combined[\"Mean_GPA\"] < 2.75]\n",
    "\n",
    "#Finds the number of rows (students) in the dataframe of Mechanical Engineering Students under 2.75 cumulatitive college GPA.\n",
    "df_Not_Graduate_Mech_Engunder275Gpa.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228eb58",
   "metadata": {},
   "source": [
    "For the Mechanical Engineering, 82% of the students who did not graduate had a mean gpa of under 2.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds a dataframe of Mechanical Engineering Students under 2.75 cumulatitive college GPA.\n",
    "df_Not_Graduate_Mech_Engunder225Gpa = df_Not_Graduate_Mech_Eng.loc[df_combined[\"Mean_GPA\"] < 2.25]\n",
    "\n",
    "#Finds the number of rows (students) in the dataframe of Mechanical Engineering Students under 2.25 cumulatitive college GPA.\n",
    "df_Not_Graduate_Mech_Engunder225Gpa.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10b9d8",
   "metadata": {},
   "source": [
    "For the Mechanical Engineering major, 63% of the students who did not graduate had a mean gpa of under 2.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds a dataframe of Mechanical Engineering Students under 2.0 cumulatitive college GPA.\n",
    "df_Not_Graduate_Mech_Engunder2Gpa = df_Not_Graduate_Mech_Eng.loc[df_combined[\"Mean_GPA\"] < 2]\n",
    "\n",
    "\n",
    "#Finds the number of rows (students) in the dataframe of Mechanical Engineering Students under 2.0 cumulatitive college.\n",
    "df_Not_Graduate_Mech_Engunder2Gpa.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad2e6f",
   "metadata": {},
   "source": [
    "For the students Mechanical Engineering major, 48.5% of the students who did not graduate had a mean gpa of under 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d04bd1",
   "metadata": {},
   "source": [
    "#### End of Slide 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.retrn_3yr.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_combined['retrn_3yr'],df_combined['grad_3yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfac669",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combined.retrn_4yr.value_counts())\n",
    "(pd.crosstab(df_combined['retrn_4yr'],df_combined['grad_4yr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combined.retrn_5yr.value_counts())\n",
    "pd.crosstab(df_combined['retrn_5yr'],df_combined['grad_5yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66d3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.retrn_5yr.value_counts()\n",
    "pd.crosstab(df_combined['retrn_6yr'],df_combined['grad_6yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a97708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in grad_data.columns:\n",
    "#     plt.figure(figsize=(10,4))\n",
    "#     sns.countplot(grad_data[i], palette = 'hls')\n",
    "#     #sns.countplot(df_retrn_data[i], data = , palette = 'hls')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bef5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyzing the dataset\n",
    "advert_report = sv.analyze(df_combined)# importing sweetviz\n",
    "#display the report\n",
    "advert_report.show_html('grade.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad74ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking amount of null values in the data\n",
    "df_combined_short_Wo_Grad.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droping PIDM as it is no longer need now that the data has been combined\n",
    "df_combined = df_combined.drop('PIDM', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737013d9",
   "metadata": {},
   "source": [
    "### Machine Learning Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa2cc2",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform non-numeric columns into numerical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for column in df_combined.columns:\n",
    "        if df_combined[column].dtype == np.number:\n",
    "            continue\n",
    "        df_combined[column] = LabelEncoder().fit_transform(df_combined[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d803cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframes for students that did return the previous year\n",
    "\n",
    "#use for return 3rd year\n",
    "df_combined_retrn_2yr_1=df_combined[df_combined['retrn_2yr']==1]\n",
    "\n",
    "\n",
    "#use for return 4th year\n",
    "df_combined_retrn_3yr_1=df_combined_retrn_2yr_1[df_combined_retrn_2yr_1['retrn_3yr']==1]\n",
    "\n",
    "\n",
    "#use for return 5th year\n",
    "df_combined_retrn_4yr_1=df_combined_retrn_3yr_1[df_combined_retrn_3yr_1['retrn_4yr']==1]\n",
    "\n",
    "\n",
    "#use for return 6th year\n",
    "df_combined_retrn_5yr_1=df_combined_retrn_4yr_1[df_combined_retrn_4yr_1['retrn_5yr']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d845a15",
   "metadata": {},
   "source": [
    "#### Checking Absolute value of Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c6521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_corr(series):\n",
    "    \n",
    "    # Get the absolute values of the Series\n",
    "    abs_series = series.abs()\n",
    "\n",
    "    # Sort the Series by absolute value in descending order\n",
    "    sorted_series = abs_series.sort_values(ascending=False)\n",
    "\n",
    "    return sorted_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e90d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrn_2yr correlations\n",
    "\n",
    "order_corr(df_combined.iloc[:, :-20].corr()['retrn_2yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c183db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrn_3yr correlations\n",
    "\n",
    "order_corr(df_combined_retrn_2yr_1.iloc[:, :-15].corr()['retrn_3yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42770c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrn_4yr correlations\n",
    "\n",
    "order_corr(df_combined_retrn_3yr_1.iloc[:, :-10].corr()['retrn_4yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrn_5yr correlations\n",
    "\n",
    "order_corr(df_combined_retrn_4yr_1.iloc[:, :-5].corr()['retrn_5yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aaee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrn_6yr correlations\n",
    "\n",
    "order_corr(df_combined_retrn_5yr_1.corr()['retrn_6yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that runs the requested algorithm and returns the accuracy metrics\n",
    "def fit_ml_algo(algo, X_train,y_train, cv):\n",
    "    \n",
    "    # One Pass\n",
    "    model = algo.fit(X_train, y_train)\n",
    "    acc = round(model.score(X_train, y_train) * 100, 2)\n",
    "    \n",
    "    # Cross Validation \n",
    "    train_pred = model_selection.cross_val_predict(algo,X_train,y_train,cv=cv,n_jobs = -1)\n",
    "    \n",
    "    # Cross-validation accuracy metric\n",
    "    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n",
    "    \n",
    "    return train_pred, acc, acc_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f9a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def lots_of_models(X_train,y_train):\n",
    "\n",
    "    # Logistic Regression\n",
    "    train_pred_log, acc_log, acc_cv_log = fit_ml_algo(LogisticRegression(random_state=123), X_train, y_train, 10)\n",
    "\n",
    "    # Support Vector Machine\n",
    "    train_pred_svc, acc_svc, acc_cv_svc = fit_ml_algo(SVC(random_state=123), X_train, y_train, 10)\n",
    "\n",
    "    # K Nearest Neighbour\n",
    "    train_pred_knn, acc_knn, acc_cv_knn = fit_ml_algo(KNeighborsClassifier(n_neighbors = 3), X_train, y_train, 10)\n",
    "\n",
    "    # Gaussian Naive Bayes\n",
    "    train_pred_gaussian, acc_gaussian, acc_cv_gaussian = fit_ml_algo(GaussianNB(), X_train, y_train, 10)\n",
    "\n",
    "    # Decision Tree\n",
    "    train_pred_dt, acc_dt, acc_cv_dt = fit_ml_algo(DecisionTreeClassifier(random_state=123), X_train, y_train, 10)\n",
    "\n",
    "    # Random Forest\n",
    "    train_pred_rf, acc_rf, acc_cv_rf = fit_ml_algo(RandomForestClassifier(n_estimators=100,random_state=123), X_train, y_train, 10)\n",
    "\n",
    "    # Gradient Boosting Trees\n",
    "    train_pred_gbt, acc_gbt, acc_cv_gbt = fit_ml_algo(GradientBoostingClassifier(random_state=123), X_train, y_train, 10)\n",
    "\n",
    "    # Perceptron\n",
    "    train_pred_perceptron, acc_perceptron, acc_cv_perceptron = fit_ml_algo(Perceptron(random_state=123), X_train, y_train, 10)\n",
    "\n",
    "    models = pd.DataFrame({\n",
    "        'Model': ['Logistic Regression','SVM','KNN','Naive Bayes','Decision Tree','Random Forest','Gradient Boosting','Perceptron'],\n",
    "        'Acc_CV': [acc_cv_log, acc_cv_svc, acc_cv_knn, acc_cv_gaussian, acc_cv_dt, acc_cv_rf, acc_cv_gbt, acc_cv_perceptron]})\n",
    "    \n",
    "    models_sorted = models.sort_values(by='Acc_CV', ascending=False)\n",
    "\n",
    "    return models_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62f4176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Grid Search \n",
    "\n",
    "def grid_search_rf(rf_params,X_train,y_train): \n",
    "\n",
    "    # Create a dictionary to store the best parameters\n",
    "    best_params = {}\n",
    "\n",
    "    # Perform grid search cross-validation for Random Forest\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=123)\n",
    "    grid_search_rf = GridSearchCV(rf, rf_params, cv=5,n_jobs=7)\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    best_params['Random Forest'] = grid_search_rf.best_params_\n",
    "\n",
    "    # Random Forest\n",
    "    train_pred_rf, acc_rf, acc_cv_rf= fit_ml_algo(RandomForestClassifier(**best_params['Random Forest'],random_state=123),X_train, y_train,10)\n",
    "\n",
    "    models = pd.DataFrame({\n",
    "        'Model': ['Random Forest'],\n",
    "        'Acc_CV': [acc_cv_rf]\n",
    "    })\n",
    "\n",
    "    return models, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradiant Boost Grid Search \n",
    "\n",
    "def grid_search_gbt(rf_params,X_train,y_train): \n",
    "\n",
    "    # Create a dictionary to store the best parameters for each model\n",
    "    best_params = {}\n",
    "    \n",
    "    # Perform grid search cross-validation for Gradient Boost\n",
    "    gb = GradientBoostingClassifier(random_state=123)\n",
    "    grid_search_gb = GridSearchCV(gb, gb_params, cv=5,n_jobs=7)\n",
    "    grid_search_gb.fit(X_train, y_train)\n",
    "    best_params['Gradient Boosting'] = grid_search_gb.best_params_\n",
    "\n",
    "    # Gradient Boosting\n",
    "    train_pred_gbt, acc_gbt, acc_cv_gbt = fit_ml_algo(GradientBoostingClassifier(**best_params['Gradient Boosting'],random_state=123),X_train, y_train,10)\n",
    "\n",
    "\n",
    "    models = pd.DataFrame({\n",
    "        'Model': ['Gradient Boosting'],\n",
    "        'Acc_CV': [acc_cv_gbt]\n",
    "    })\n",
    "    return models, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263485b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Grid Search \n",
    "\n",
    "def grid_search_svm(svm_params, X_train, y_train):\n",
    "\n",
    "    # Create a dictionary to store the best parameters for each model\n",
    "    best_params = {}\n",
    "    \n",
    "    # Perform grid search cross-validation for SVM\n",
    "    svm = SVC(random_state=123)\n",
    "    grid_search_svm = GridSearchCV(svm, svm_params, cv=5, n_jobs=7)\n",
    "    grid_search_svm.fit(X_train, y_train)\n",
    "    best_params['Support Vector Machine'] = grid_search_svm.best_params_\n",
    "    \n",
    "    # SVM\n",
    "    train_pred_svm, acc_svm, acc_cv_svm = fit_ml_algo(SVC(**best_params['Support Vector Machine'],random_state=123),X_train, y_train,10)\n",
    "\n",
    "\n",
    "    models = pd.DataFrame({\n",
    "        'Model': ['Support Vector Machine'],\n",
    "        'Acc_CV': [acc_cv_svm]\n",
    "    })\n",
    "    return models, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "### KNN Grid Search \n",
    "\n",
    "def grid_search_knn(knn_params):\n",
    "    \n",
    "    # Create a dictionary to store the best parameters for each model\n",
    "    best_params = {}\n",
    "    \n",
    "    # Perform grid search cross-validation for KNN\n",
    "    knn = KNeighborsClassifier()\n",
    "    grid_search_knn = GridSearchCV(knn, knn_params, cv=5, n_jobs=7)\n",
    "    grid_search_knn.fit(X_train, y_train)\n",
    "    best_params['K Nearest Neighbour'] = grid_search_knn.best_params_\n",
    "\n",
    "    # K-Nearest Neighbors\n",
    "    train_pred_knn, acc_knn, acc_cv_knn = fit_ml_algo(KNeighborsClassifier(**best_params['K Nearest Neighbour']), X_train, y_train, 10)\n",
    "\n",
    "    models = pd.DataFrame({\n",
    "        'Model': ['K-Nearest Neighbors'],\n",
    "        'Acc_CV': [acc_cv_knn]\n",
    "    })\n",
    "    return models, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf3e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_lr(lr_params):\n",
    "\n",
    "    # Create an instance of LogisticRegression\n",
    "    logreg = LogisticRegression(random_state=123)\n",
    "\n",
    "    # Perform grid search cross-validation for logistic regression\n",
    "    grid_search_logreg = GridSearchCV(logreg, lr_params, cv=5, n_jobs=7)\n",
    "    grid_search_logreg.fit(X_train, y_train)\n",
    "\n",
    "    # Extract the best parameters and fit the logistic regression model with the best parameters\n",
    "    best_params['Logistic Regression'] = grid_search_logreg.best_params_\n",
    "\n",
    "    \n",
    "    # Logistic Regression\n",
    "    train_pred_log, acc_log, acc_cv_log = fit_ml_algo(LogisticRegression(**best_params['Logistic Regression'],random_state=123), X_train, y_train, 10)\n",
    "\n",
    "\n",
    "    models = pd.DataFrame({\n",
    "        'Model': ['Logistic Regression'],\n",
    "        'Acc_CV': [acc_cv_log]\n",
    "    })\n",
    "\n",
    "    return models, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bcad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_rf(best_params):\n",
    "    # Create a RandomForestClassifier with the specified hyperparameters\n",
    "    \n",
    "    rf = RandomForestClassifier(**best_params['Random Forest'],random_state=123)\n",
    "\n",
    "    # Fit the RandomForestClassifier to your data\n",
    "    rf.fit(X_train, y_train)  # Replace X_train and y_train with your actual training data\n",
    "\n",
    "    # Get feature importances as a numpy array\n",
    "    importances = rf.feature_importances_\n",
    "\n",
    "    # Create a dataframe to store the feature importances\n",
    "    df_feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "\n",
    "    # Sort the dataframe by feature importance in descending order\n",
    "    df_feature_importance = df_feature_importance.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    return df_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca31e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_gbt(best_params):\n",
    "    \n",
    "    # Create a GradientBoostingClassifier with the specified hyperparameters\n",
    "    gbt = GradientBoostingClassifier(**best_params['Gradient Boosting'],random_state=123)\n",
    "\n",
    "    # Fit the RandomForestClassifier to your data\n",
    "    gbt.fit(X_train, y_train)  # Replace X_train and y_train with your actual training data\n",
    "\n",
    "    # Get feature importances as a numpy array\n",
    "    importances = gbt.feature_importances_\n",
    "\n",
    "    # Create a dataframe to store the feature importances\n",
    "    df_feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "\n",
    "    # Sort the dataframe by feature importance in descending order\n",
    "    df_feature_importance = df_feature_importance.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    return df_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6974f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_svm(best_params):\n",
    "    # Create a Support Vector Machine (SVM) classifier with the specified hyperparameters\n",
    "    svm = SVC(**best_params['Support Vector Machine'],random_state=123)\n",
    "\n",
    "    # Fit the SVM classifier to your data\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate permutation importances\n",
    "    result = permutation_importance(svm, X_train, y_train, n_repeats=10, random_state=0)\n",
    "    importances = result.importances_mean\n",
    "\n",
    "    # Create a dataframe to store the feature importances\n",
    "    df_feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "\n",
    "    # Sort the dataframe by feature importance in descending order\n",
    "    df_feature_importance = df_feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return df_feature_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9903d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_knn(best_params):\n",
    "    # Create a KNN classifier with the specified hyperparameters\n",
    "    knn = KNeighborsClassifier(**best_params['K Nearest Neighbour'])\n",
    "\n",
    "    # Fit the KNN classifier to your data\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate permutation importances\n",
    "    result = permutation_importance(knn, X_train, y_train, n_repeats=10, random_state=0)\n",
    "    importances = result.importances_mean\n",
    "\n",
    "    # Create a dataframe to store the feature importances\n",
    "    df_feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "\n",
    "    # Sort the dataframe by feature importance in descending order\n",
    "    df_feature_importance = df_feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return df_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de17a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_lr(best_params):\n",
    "\n",
    "    # Create an instance of LogisticRegression with the best hyperparameters\n",
    "    logreg = LogisticRegression(**best_params['Logistic Regression'],random_state=123)\n",
    "\n",
    "    # Fit the logistic regression model to the training data\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    # Get coefficients as feature importances\n",
    "    importances = logreg.coef_[0]\n",
    "\n",
    "    # Create a dataframe to store the feature importances\n",
    "    df_feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "\n",
    "    # Sort the dataframe by feature importance in descending order\n",
    "    df_feature_importance = df_feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cb251",
   "metadata": {},
   "source": [
    "### Predictive Models for 2nd Year Student Return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c497f0c",
   "metadata": {},
   "source": [
    "#### 2nd Year Intitial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "#Feature Selection from correlation\n",
    "X=X.drop(['median_household_income'], axis =1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X=X.iloc[:, :-20]\n",
    "\n",
    "#Y value\n",
    "Y_2yr=df_combined['retrn_2yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_2yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d592513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f3276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#checking feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad91bfca",
   "metadata": {},
   "source": [
    "This random forest model that predicts 2nd-year returns is very accurate with an accuracy of 97.78%. However, the imbalance of feature importance, with Num_of_Crses being overwhelmingly greater than the other features, could indicate that some of the other features should be removed to create an improved model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c259374",
   "metadata": {},
   "source": [
    "#### 2nd Year Model using Top Feature Importance Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df3468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "selected_features = df_feature_importance.head(8)['Feature'].values\n",
    "\n",
    "#X value\n",
    "X = df_combined.loc[:, selected_features]\n",
    "\n",
    "#Y value\n",
    "Y_2yr=df_combined['retrn_2yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_2yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846706a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#checking feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5944c07",
   "metadata": {},
   "source": [
    "This model had the same accuracy as 97.78%. This meant that there was no notable increase in the accuracy even with the removal of potential noise from low feature importance columns. Interestingly the model seems to put even greater emphasis on Num_of_Crses, which could indicate that an even simpler model may be all that is required to create an accurate model on 2nd-year return students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a03f8f",
   "metadata": {},
   "source": [
    "#### 2nd Year Model Excluding Num_of_Crses from X Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73363f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "#Feature Selection from correlation\n",
    "X=X.drop(['median_household_income','Num_of_Crses'], axis =1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X=X.iloc[:, :-20]\n",
    "\n",
    "#Y value\n",
    "Y_2yr=df_combined['retrn_2yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_2yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c297ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae531841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking feature importance of first model \n",
    "\n",
    "gb_params = {\n",
    "    'n_estimators': [50,75,100, 200, 300], \n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7], \n",
    "    'min_samples_split': [2, 4, 8], \n",
    "    'min_samples_leaf': [1, 2, 4] \n",
    "}\n",
    "\n",
    "models, best_params = grid_search_gbt(gb_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#checking feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_gbt(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339d9c3",
   "metadata": {},
   "source": [
    "This model reconfirms my suspicion that the feature Num_of_Crses plays a vital role in creating an optimal model since the accuracy in this model, 89.63%, is significantly less than the previous models that did use Num_of_Crses. This model does interestingly show that it places great importance on the Mean_GPA feature, so the importance of this feature will be checked upon in the next model that removes this feature as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a986808f",
   "metadata": {},
   "source": [
    "####  2nd Year Model Excluding Num_of_Crses and Mean_GPA from X Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa97d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "#Feature Selection from correlation\n",
    "X=X.drop(['Mean_GPA','Num_of_Crses'], axis =1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X=X.iloc[:, :-20]\n",
    "\n",
    "#Y value\n",
    "Y_2yr=df_combined['retrn_2yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_2yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595ab481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3621c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking feature importance of first model \n",
    "\n",
    "gb_params = {\n",
    "    'n_estimators': [50,75,100, 200, 300], \n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7], \n",
    "    'min_samples_split': [2, 4, 8], \n",
    "    'min_samples_leaf': [1, 2, 4] \n",
    "}\n",
    "\n",
    "models, best_params = grid_search_gbt(gb_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#feature importance\n",
    "df_feature_importance=feature_importance_gbt(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51b442",
   "metadata": {},
   "source": [
    "Again it is apparent that the model is not performing as well as the models that did include Num_of_Crses. This again reconfirms that Num_of_Crses plays a notable role in the creation of an optimal model to predict students returning during their 2nd year. Another interesting finding is despite Mean_GPA seemingly like another very important feature in previous models, the removal of the feature did not greatly decrease this model's accuracy relative to the previous model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c7664",
   "metadata": {},
   "source": [
    "#### 2nd Year Model with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eabcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis=1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X = X.iloc[:, :-20]\n",
    "\n",
    "#PCA\n",
    "columns_for_pca = X.columns.tolist()\n",
    "columns_for_pca.remove('Num_of_Crses')\n",
    "columns_for_pca.remove('Mean_GPA')\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=4)\n",
    "X_pca = pca.fit_transform(X[columns_for_pca])\n",
    "\n",
    "# Create new columns in X for the PCA results\n",
    "for i in range(4):\n",
    "    X[f'PCA_Component_{i+1}'] = X_pca[:,i]\n",
    "\n",
    "# Drop the original columns used for PCA\n",
    "X.drop(columns=columns_for_pca, inplace=True, axis=1)\n",
    "\n",
    "#Y value\n",
    "Y_2yr = df_combined['retrn_2yr']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_2yr, test_size=0.20, random_state=123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb1d0fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tunning hyperparameters in gradiant boost\n",
    "\n",
    "gb_params = {\n",
    "    'n_estimators': [50,75,100, 200, 300], \n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7], \n",
    "    'min_samples_split': [2, 4, 8], \n",
    "    'min_samples_leaf': [1, 2, 4] \n",
    "}\n",
    "\n",
    "models, best_params = grid_search_gbt(gb_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_gbt(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7aa84b",
   "metadata": {},
   "source": [
    "To reduce the noise from the lower-importance features in the data, PCA was used. The model that used PCA had an accuracy of 97.61% which meant it did a lot better than the last two models. However, it did do a bit worse than the first two models. This makes it very apparent that Num_of_Crses may be by far the single most important feature in predicting 2nd-year student return. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9ba24",
   "metadata": {},
   "source": [
    "#### 2nd Year Model Using Only Num_of_Crses and Mean_GPA for X Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a35469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined[['Num_of_Crses', 'Mean_GPA']]\n",
    "\n",
    "#Y value\n",
    "Y_2yr=df_combined['retrn_2yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_2yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637bf4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e4cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in SVM\n",
    "\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    'kernel': ['rbf','linear']}\n",
    "\n",
    "models, best_params = grid_search_svm(svm_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_svm(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa1c6ca",
   "metadata": {},
   "source": [
    "Since Num_of_Crses and Mean_GPA seemed to be the most important features in model performance, this model used only those two features. This model performed better than all other models with an accuracy of 97.87%. Since this model is also the simplest model and easiest to explain, this model will be chosen to be our final model for our model to predict 2nd year return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245f1b9",
   "metadata": {},
   "source": [
    "#### Saving the Best Performing Model for 2nd Year Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51252da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined[['Num_of_Crses', 'Mean_GPA']]\n",
    "\n",
    "#Y value\n",
    "Y_2yr=df_combined['retrn_2yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_2yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "#Inputting hyper-parameters\n",
    "svc_model = SVC(**{'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'},random_state=123)\n",
    "\n",
    "#recreating best model\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "#confusion matrix\n",
    "y_pred = svc_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix\")\n",
    "print(cm)\n",
    "\n",
    "\n",
    "joblib.dump(svc_model, '2nd_year_return_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f05360",
   "metadata": {},
   "source": [
    "### Predictive Models for 3rd Year Student Return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03854649",
   "metadata": {},
   "source": [
    "#### 3rd Year Intitial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f125d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_2yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X=X.iloc[:, :-15]\n",
    "\n",
    "#Y value\n",
    "Y_3yr=df_combined_retrn_2yr_1['retrn_3yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_3yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57dadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#checking feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d4354a",
   "metadata": {},
   "source": [
    "The random forest model that predicts 3rd-year returns is very accurate with an accuracy of 96.53%. However, the imbalance of feature importance, with Num_of_Crses being overwhelmingly greater than the other features, could indicate that some of the other features should be removed to create an improved model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9248c",
   "metadata": {},
   "source": [
    "#### 3rd year  Model using Top Feature Importance Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4057daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "selected_features = df_feature_importance.head(14)['Feature'].values\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_2yr_1.loc[:, selected_features]\n",
    "\n",
    "#Y value\n",
    "Y_3yr=df_combined_retrn_2yr_1['retrn_3yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_3yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")\n",
    "\n",
    "\n",
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1213d0b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#checking feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a381c",
   "metadata": {},
   "source": [
    "This  model achieved an accuracy of 96.42%, which is slightly lower than expected. However, even after limiting the analysis to the top feature importance values, the difference in accuracy was not significantly different between this model and the intial model. This model further emphasizes the critical role of the Num_of_Crses feature, which had a feature importance score that was over 40 times higher than the next most important feature. To confirm the crucial nature of this feature, the next model will exclude \"Num_of_Crses\" to assess its impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255d15f",
   "metadata": {},
   "source": [
    "#### 3rd year model with no Num_of_Crses in X values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5fca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_2yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "#Feature Selection from correlation\n",
    "X=X.drop(['Num_of_Crses'], axis =1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X=X.iloc[:, :-15]\n",
    "\n",
    "#Y value\n",
    "Y_3yr=df_combined_retrn_2yr_1['retrn_3yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_3yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13adda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#checking feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e580e",
   "metadata": {},
   "source": [
    "With an accuracy of 92.34%, this model performed significantly worse than the previous two models. Interestingly, the feature importance analysis revealed that the Mean_GPA feature had a substantial impact on the model's performance, suggesting its potential importance in creating an optimized model. To further investigate this feature's significance, the next model will exclude Mean_GPA to assess the impact of its removal on the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc6010",
   "metadata": {},
   "source": [
    "#### 3rd year model with no Num_of_Crses, Mean_GPA in X values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c85740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_2yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "#Feature Selection from correlation\n",
    "X=X.drop(['Mean_GPA','Num_of_Crses'], axis =1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X=X.iloc[:, :-15]\n",
    "\n",
    "#Y value\n",
    "Y_3yr=df_combined_retrn_2yr_1['retrn_3yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_3yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb01bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#checking feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21874a6a",
   "metadata": {},
   "source": [
    "The accuracy of this model was 91.01%, which indicates a slight decrease in performance compared to the previous model. The drop in accuracy is approximately 1%. This observation suggests that Mean_GPA might still be a valuable feature to include in future models to optimize their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859ca03",
   "metadata": {},
   "source": [
    "#### 3rd year model with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76674619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_2yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis=1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X = X.iloc[:, :-15]\n",
    "\n",
    "#PCA\n",
    "columns_for_pca = X.columns.tolist()\n",
    "columns_for_pca.remove('Num_of_Crses')\n",
    "columns_for_pca.remove('Mean_GPA')\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=4)\n",
    "X_pca = pca.fit_transform(X[columns_for_pca])\n",
    "\n",
    "# Create new columns in X for the PCA results\n",
    "for i in range(4):\n",
    "    X[f'PCA_Component_{i+1}'] = X_pca[:,i]\n",
    "\n",
    "# Drop the original columns used for PCA\n",
    "X.drop(columns=columns_for_pca, inplace=True, axis=1)\n",
    "\n",
    "#Y value\n",
    "Y_3yr = df_combined_retrn_2yr_1['retrn_3yr']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_3yr, test_size=0.20, random_state=123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df072512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#checking feature importance of first model \n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ec724",
   "metadata": {},
   "source": [
    "This model achieved an accuracy of 96.63%, which is the highest accuracy obtained thus far. Although the improvement over the first two models is not significant, it is still a noteworthy advancement considering the relatively small increase of only 0.1% in accuracy. Additionally, the feature importance analysis indicates that the most valuable features are Num_of_Crses and Mean_GPA, which have much higher importance scores compared to the other features. This finding will be further investigated in the next model, which will exclusively use Num_of_Crses and Mean_GPA to develop the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e819024b",
   "metadata": {},
   "source": [
    "#### 3rd year Model with only Num_of_Crses and Mean_GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57aedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_2yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "#Feature Selection from correlation\n",
    "X=df_combined_retrn_2yr_1[['Mean_GPA','Num_of_Crses']]\n",
    "\n",
    "#Y value\n",
    "Y_3yr=df_combined_retrn_2yr_1['retrn_3yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_3yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af78f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in SVM\n",
    "\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    'kernel': ['rbf','linear']}\n",
    "\n",
    "models, best_params = grid_search_svm(svm_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_svm(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f5575f",
   "metadata": {},
   "source": [
    "Since Num_of_Crses and Mean_GPA seemed to be the most important features in model performance, this model used only those two features. The model achieved an accuracy of 96.73%, surpassing the performance of all other models. Furthermore, this model is the simplest and easiest to explain, making it an ideal choice as the final model to predict 3rd-year return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352d603",
   "metadata": {},
   "source": [
    "#### Saving the Best Performing Model for 3rd Year Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd632c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_2yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "#Feature Selection from correlation\n",
    "X=df_combined_retrn_2yr_1[['Mean_GPA','Num_of_Crses']]\n",
    "\n",
    "#Y value\n",
    "Y_3yr=df_combined_retrn_2yr_1['retrn_3yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_3yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "#Inputting hyper-parameters\n",
    "svc_model = SVC(**{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'},random_state=123)\n",
    "\n",
    "#recreating model\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "#confusion matrix\n",
    "y_pred = svc_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix\")\n",
    "print(cm)\n",
    "\n",
    "joblib.dump(svc_model, '3rd_year_return_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f959e10",
   "metadata": {},
   "source": [
    "### Predictive Models for 4th Year Student Return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5b32b",
   "metadata": {},
   "source": [
    "#### 4th Year Intitial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_3yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X=X.iloc[:, :-10]\n",
    "\n",
    "#Y value\n",
    "Y_4yr=df_combined_retrn_3yr_1['retrn_4yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_4yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03624b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#checking feature importance \n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65718a",
   "metadata": {},
   "source": [
    "The random forest model that predicts 2nd-year returns is very accurate with an accuracy of 96.24%. However, the imbalance of feature importance, with Num_of_Crses being overwhelmingly greater than the other features, indicating that the removal of other features could aid in creating a more optimal model. So a filter of the highest feature importances will be used in the next model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f6ee7",
   "metadata": {},
   "source": [
    "#### 4th year return Model with feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "selected_features = df_feature_importance.head(8)['Feature'].values\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_3yr_1.loc[:, selected_features]\n",
    "\n",
    "#Y value\n",
    "Y_4yr=df_combined_retrn_3yr_1['retrn_4yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_4yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a9ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#checking feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec431ce",
   "metadata": {},
   "source": [
    "Despite the removal of a large set of features, this model had the same accuracy as the previous model. This model had an accuracy of 96.24%. The model also had two features that stood out as having significant higher values in their feature importance than other features which were the features Num_of_Crses and Mean_GPA. In order to further check on the impact of these two features the Num_of_Crses will be removed to see how a model trained without that feature would perform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae3c2f",
   "metadata": {},
   "source": [
    "#### 4th year model with no Num_of_Crses in X values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_3yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "#Testing no Num_of_Crses\n",
    "X=X.drop(['Num_of_Crses'], axis =1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X=X.iloc[:, :-10]\n",
    "\n",
    "#Y value\n",
    "Y_4yr=df_combined_retrn_3yr_1['retrn_4yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_4yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e754818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40362591",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#checking feature importance \n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f72179",
   "metadata": {},
   "source": [
    "This trained model had a accuracy of 93.61%. As with the previous year models, when removing the Num_of_Crses, the model has a worse performance than models that do have this feature. This further reinforces the idea that Num_of_Crses plays a critical role in creating an optimal model in predicting student reuturn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fa15f",
   "metadata": {},
   "source": [
    "#### 4th year model with no Num_of_Crses, Mean_GPA in X values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4828e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_3yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "#Testing no Num_of_Crses\n",
    "X=X.drop(['Mean_GPA','Num_of_Crses'], axis =1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X=X.iloc[:, :-10]\n",
    "\n",
    "#Y value\n",
    "Y_4yr=df_combined_retrn_3yr_1['retrn_4yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_4yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c4d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking feature importance of RF\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#checking feature importance \n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dbbf0c",
   "metadata": {},
   "source": [
    "The accuracy of this model was 93.84%, which is the lowest among all the 4th year return models. It is evident yet again that the model's performance is inferior to the models that included both Num_of_Crses and Mean_GPA. This finding reinforces the notion that Num_of_Crses plays a significant role in creating an optimal model to predict students returning during their 4th year, and although not as critical, Mean_GPA also plays a notable role in improving the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115de71",
   "metadata": {},
   "source": [
    "#### 4th year Model with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_3yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X = X.iloc[:, :-10]\n",
    "\n",
    "#PCA\n",
    "columns_for_pca = X.columns.tolist()\n",
    "columns_for_pca.remove('Num_of_Crses')\n",
    "columns_for_pca.remove('Mean_GPA')\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X[columns_for_pca])\n",
    "\n",
    "# Create new columns in X for the PCA results\n",
    "for i in range(5):\n",
    "    X[f'PCA_Component_{i+1}'] = X_pca[:,i]\n",
    "\n",
    "# Drop the original columns used for PCA\n",
    "X.drop(columns=columns_for_pca, inplace=True, axis=1)\n",
    "\n",
    "#Y value\n",
    "Y_4yr=df_combined_retrn_3yr_1['retrn_4yr']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_4yr, test_size=0.20, random_state=123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307302ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1644b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621860c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in gradiant boost\n",
    "\n",
    "gb_params = {\n",
    "    'n_estimators': [75,100,200], \n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7], \n",
    "    'min_samples_split': [2, 4, 8], \n",
    "    'min_samples_leaf': [1, 2, 4] \n",
    "}\n",
    "\n",
    "models, best_params = grid_search_gbt(gb_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_gbt(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0722b824",
   "metadata": {},
   "source": [
    "When applying PCA to reduce noise in the model, the resulting accuracy was 95.32%, slightly lower than that of the original two models. The feature importance analysis of this model reaffirmed that Num_of_Crses remains a crucial determinant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b11d1",
   "metadata": {},
   "source": [
    "#### 4th year Model with only Num_of_Crses and Mean_GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X=df_combined_retrn_3yr_1[['Mean_GPA','Num_of_Crses']]\n",
    "\n",
    "\n",
    "#Y value\n",
    "Y_4yr=df_combined_retrn_3yr_1['retrn_4yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_4yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f27b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baca2e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tunning hyperparameters in KNN\n",
    "\n",
    "knn_params = { 'n_neighbors' : [2,3,5,7,9],\n",
    "               'weights' : ['uniform','distance'],\n",
    "               'metric' : ['minkowski','euclidean','manhattan']}\n",
    "\n",
    "models, best_params = grid_search_knn(knn_params)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_knn(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c270241",
   "metadata": {},
   "source": [
    "By utilizing only the two features, Num_of_Crses and Mean_GPA, to create a predictive model, I achieved an accuracy of 96.12%. This is only marginally lower than the first two models, with a difference of just 0.1%. Despite its slightly lower accuracy, I will opt to use this model, applying the principle of Occam's razor, as a K Nearest Neighbour model is simpler than the Random Forest models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9b13fa",
   "metadata": {},
   "source": [
    "#### Saving the Best Performing Model for 4th Year Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X=df_combined_retrn_3yr_1[['Mean_GPA','Num_of_Crses']]\n",
    "\n",
    "\n",
    "#Y value\n",
    "Y_4yr=df_combined_retrn_3yr_1['retrn_4yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_4yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "#Inputting hyper-parameters\n",
    "knn_model = KNeighborsClassifier(**{'metric': 'minkowski','n_neighbors': 7,'weights': 'uniform'})\n",
    "\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "#confusion matrix\n",
    "y_pred = knn_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix\")\n",
    "print(cm)\n",
    "\n",
    "joblib.dump(knn_model, '4th_year_return_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc794c6",
   "metadata": {},
   "source": [
    "### Predictive Models for 2nd Year Student Return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba09f0",
   "metadata": {},
   "source": [
    "#### 5th Year Intitial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a2afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_4yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X=X.iloc[:, :-5]\n",
    "\n",
    "#Y value\n",
    "Y_5yr=df_combined_retrn_4yr_1['retrn_5yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_5yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff42cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5934d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf4fcc",
   "metadata": {},
   "source": [
    "The intial model for predicting 5th year return has the worst performance out of all the previous models when predicting their return with a 80.17% accuracy. All the previous models had an accuracy of 90% and higher. This is a slight concern as this could indicate issues with the data used for modeling for 5th year return or it could just mean that the features are not a great predictor on student reutrn. Inorder to further investigate this issue a filter of the top feature importance features will be used to make a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53289f1",
   "metadata": {},
   "source": [
    "#### 5th Year Model using Top Feature Importance Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4668d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "selected_features = df_feature_importance.head(26)['Feature'].values\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_4yr_1.loc[:, selected_features]\n",
    "\n",
    "#Y value\n",
    "Y_5yr=df_combined_retrn_4yr_1['retrn_5yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_5yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c231cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007c85b",
   "metadata": {},
   "source": [
    "By taking only the columns with higher feature importance, this model had a better accuracy with an accuracy of 81.88%. Though the increase was only around 1%, it still increased which could indicate that there may be too much noise in the model to accuractly predict the return of 5th year students. The next model will use PCA to try to tackle this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddaa900",
   "metadata": {},
   "source": [
    "#### 5th year Model with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_4yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis=1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X = X.iloc[:, :-5]\n",
    "\n",
    "#PCA\n",
    "columns_for_pca = X.columns.tolist()\n",
    "columns_for_pca.remove('Num_of_Crses')\n",
    "columns_for_pca.remove('Mean_GPA')\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=6)\n",
    "X_pca = pca.fit_transform(X[columns_for_pca])\n",
    "\n",
    "# Create a new column in X for the PCA result\n",
    "for i in range(6):\n",
    "    X[f'PCA_Component_{i+1}'] = X_pca[:, i]\n",
    "\n",
    "# Drop the original columns used for PCA\n",
    "X.drop(columns=columns_for_pca, inplace=True)\n",
    "\n",
    "# Y value\n",
    "Y_5yr = df_combined_retrn_4yr_1['retrn_5yr']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_5yr, test_size=0.20, random_state=123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced640d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in gradiant boost\n",
    "\n",
    "gb_params = {\n",
    "    'n_estimators': [75,100,200], \n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7], \n",
    "    'min_samples_split': [2, 4, 8], \n",
    "    'min_samples_leaf': [1, 2, 4] \n",
    "}\n",
    "\n",
    "models, best_params = grid_search_gbt(gb_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_gbt(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5269c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2953f3e8",
   "metadata": {},
   "source": [
    "Contrary to expectation, the models created with PCA failed to achieve an accuracy of 80%. These models performed worse than the previous two, suggesting that there may be too much noise in the data, which even PCA was unable to remove. To improve the model's performance in predicting 5th year students, the next model will be using only Num_of_Crses and Mean_GPA as features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb15bcb1",
   "metadata": {},
   "source": [
    "#### 5th Year Model Using Only Num_of_Crses and Mean_GPA for X Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff157299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_4yr_1[['Num_of_Crses', 'Mean_GPA']]\n",
    "\n",
    "#Y value\n",
    "Y_5yr=df_combined_retrn_4yr_1['retrn_5yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_5yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in Logistic Regression\n",
    "lr_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_lr(lr_params)\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_lr(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20d09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in SVM\n",
    "\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    'kernel': ['rbf','linear']}\n",
    "\n",
    "models, best_params = grid_search_svm(svm_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_svm(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f1979",
   "metadata": {},
   "source": [
    "For the first time in all the different return year models, the models created with only Num_of_Crses and Mean_GPA as input features did not perform well. None of the models were able to achieve an accuracy of 80%, and all were outperformed by the previous models. The current method of subsetting the data may be throwing off the model, as it keeps all students who returned in the previous year. Given that most students graduate in four years, this could be adding a lot of noise and leading to subpar predictive models for 5th year return. I will save the model that utilized the features with the highest importance, as it performed the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f7814d",
   "metadata": {},
   "source": [
    "#### Saving the Best Performing Model for 5th Year Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561bfa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_4yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "# Dropping classes that Students haven't taken yet\n",
    "X=X.iloc[:, :-5]\n",
    "\n",
    "#Y value\n",
    "Y_5yr=df_combined_retrn_4yr_1['retrn_5yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_5yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "\n",
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {'bootstrap': [True],\n",
    "             'criterion': ['entropy'],\n",
    "             'max_depth': [None],\n",
    "             'max_features': [None],\n",
    "             'min_samples_leaf': [2],\n",
    "             'min_samples_split': [8],\n",
    "             'n_estimators': [50]}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "\n",
    "\n",
    "#train data split\n",
    "\n",
    "selected_features = df_feature_importance.head(26)['Feature'].values\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_4yr_1.loc[:, selected_features]\n",
    "\n",
    "#Y value\n",
    "Y_5yr=df_combined_retrn_4yr_1['retrn_5yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_5yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "#Inputting hyper-parameters\n",
    "rf_model = RandomForestClassifier(**{'bootstrap': True, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50},random_state=123)\n",
    "\n",
    "#train model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "#creating confusion matrix\n",
    "y_pred = rf_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix\")\n",
    "print(cm)\n",
    "\n",
    "#saving model\n",
    "joblib.dump(rf_model, '5th_year_return_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40749b65",
   "metadata": {},
   "source": [
    "### Predictive Models for 6th Year Student Return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1770c4c9",
   "metadata": {},
   "source": [
    "#### Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_5yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "#Y value\n",
    "Y_6yr=df_combined_retrn_5yr_1['retrn_6yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_6yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049755c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9948de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "# feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992819f1",
   "metadata": {},
   "source": [
    "The initial model for predicting 6th year return achieved an accuracy of 87.76%. Compared to all the other models, this model had the most balanced feature importance values, but it still followed the pattern seen in other models where Num_of_Crses and Mean_GPA had the highest importance values. Given the large number of features in the model, I will filter out the features with low importance and keep only the ones with the highest importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b762cad",
   "metadata": {},
   "source": [
    "####  6th Year Model using Top Feature Importance Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "selected_features = df_feature_importance.head(8)['Feature'].values\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_5yr_1.loc[:, selected_features]\n",
    "\n",
    "#Y value\n",
    "Y_6yr=df_combined_retrn_5yr_1['retrn_6yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_6yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a73d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e2d514",
   "metadata": {},
   "source": [
    "The model showed a slight improvement, with an accuracy of 89.12%, which is about 2% higher than the previous model. This suggests that the feature filtering process did improve the model performance, by removing noise caused by less important features. Additionaly the feature importance of the features to more balanced. To further improve the model by reducing noise, I will apply PCA in the next model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d930ac38",
   "metadata": {},
   "source": [
    "#### 6th year model with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f672ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_5yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis=1)\n",
    "\n",
    "\n",
    "#PCA\n",
    "columns_for_pca = X.columns.tolist()\n",
    "columns_for_pca.remove('Num_of_Crses')\n",
    "columns_for_pca.remove('Mean_GPA')\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=7)\n",
    "X_pca = pca.fit_transform(X[columns_for_pca])\n",
    "\n",
    "# Create a new column in X for the PCA result\n",
    "for i in range(7):\n",
    "    X[f'PCA_Component_{i+1}'] = X_pca[:, i]\n",
    "\n",
    "# Drop the original columns used for PCA\n",
    "X.drop(columns=columns_for_pca, inplace=True)\n",
    "\n",
    "#Y value\n",
    "Y_6yr = df_combined_retrn_5yr_1['retrn_6yr']\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_6yr, test_size=0.20, random_state=123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034481d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None,2,3,4],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'n_estimators': [1,2,4,50,75]\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83fc7c",
   "metadata": {},
   "source": [
    "Using PCA the model again out performed the intial model by about 1%. However this model did not beat the previous model that used the higest feature importance features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c311671d",
   "metadata": {},
   "source": [
    "#### 6th Year Model Using Only Num_of_Crses and Mean_GPA for X Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2360fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_5yr_1[['Num_of_Crses', 'Mean_GPA']]\n",
    "\n",
    "#Y value\n",
    "Y_6yr=df_combined_retrn_5yr_1['retrn_6yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_6yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Test Data Shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce73cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy comparison\n",
    "\n",
    "models_sorted=lots_of_models(X_train,y_train)\n",
    "\n",
    "display(models_sorted)\n",
    "\n",
    "print(models_sorted.iloc[0][0],\"is the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a05495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning hyperparameters in SVM\n",
    "\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    'kernel': ['rbf','linear']}\n",
    "\n",
    "models, best_params = grid_search_svm(svm_params,X_train,y_train)\n",
    "\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_svm(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b28399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tunning hyperparameters in Logistic Regression\n",
    "lr_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "models, best_params = grid_search_lr(lr_params)\n",
    "display(models,best_params)\n",
    "\n",
    "#Feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_lr(best_params)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abb4ad0",
   "metadata": {},
   "source": [
    "Once again, the models that used only Num_of_Crses and Mean_GPA as features performed better than the initial model. However, none of these models outperformed the model that used the top feature importance values. Therefore, I will save the model that performed the best, which is the one that used the top feature importance values, as our final model for predicting 6th year return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7658a96",
   "metadata": {},
   "source": [
    "#### Saving the Best Performing Model for 6th Year Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ec5f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data split\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_5yr_1.drop(['retrn_2yr','retrn_3yr','retrn_4yr','retrn_5yr','retrn_6yr','grad_3yr','grad_4yr','grad_5yr','grad_6yr','graduated','term_code_key'], axis =1)\n",
    "\n",
    "#Y value\n",
    "Y_6yr=df_combined_retrn_5yr_1['retrn_6yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_6yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {'bootstrap': [False],\n",
    "  'criterion': ['gini'],\n",
    "  'max_depth': [None],\n",
    "  'max_features': ['sqrt'],\n",
    "  'min_samples_leaf': [2],\n",
    "  'min_samples_split': [8],\n",
    "  'n_estimators': [4]}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "\n",
    "# feature importance\n",
    "\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "df_feature_importance\n",
    "\n",
    "\n",
    "#train data split\n",
    "\n",
    "selected_features = df_feature_importance.head(8)['Feature'].values\n",
    "\n",
    "#X value\n",
    "X = df_combined_retrn_5yr_1.loc[:, selected_features]\n",
    "\n",
    "#Y value\n",
    "Y_6yr=df_combined_retrn_5yr_1['retrn_6yr']\n",
    "\n",
    "#Split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y_6yr, test_size=0.20, random_state = 123)\n",
    "\n",
    "#tunning hyperparameters in random forest model\n",
    "\n",
    "rf_params = {'bootstrap': [True],\n",
    "  'criterion': ['gini'],\n",
    "  'max_depth': [4],\n",
    "  'max_features': ['log2'],\n",
    "  'min_samples_leaf': [1],\n",
    "  'min_samples_split': [4],\n",
    "  'n_estimators': [75]}\n",
    "\n",
    "models, best_params = grid_search_rf(rf_params,X_train,y_train)\n",
    "\n",
    "#Feature importance\n",
    "df_feature_importance=feature_importance_rf(best_params)\n",
    "\n",
    "\n",
    "#Inputting hyper-parameters\n",
    "rf_model = RandomForestClassifier(**best_params['Random Forest'],random_state=123)\n",
    "\n",
    "#Recreating model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "#creating confusion matrix\n",
    "y_pred = rf_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix\")\n",
    "print(cm)\n",
    "\n",
    "joblib.dump(rf_model, '6th_year_return_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
